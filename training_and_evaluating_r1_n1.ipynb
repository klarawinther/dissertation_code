{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>sample_location_id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05020</td>\n",
       "      <td>0.09065</td>\n",
       "      <td>0.13930</td>\n",
       "      <td>0.15855</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.19205</td>\n",
       "      <td>0.19175</td>\n",
       "      <td>0.20290</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.03980</td>\n",
       "      <td>0.05720</td>\n",
       "      <td>0.10385</td>\n",
       "      <td>0.16755</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.21600</td>\n",
       "      <td>0.23420</td>\n",
       "      <td>0.24700</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01510</td>\n",
       "      <td>0.02905</td>\n",
       "      <td>0.06635</td>\n",
       "      <td>0.04515</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.38280</td>\n",
       "      <td>0.47410</td>\n",
       "      <td>0.49955</td>\n",
       "      <td>0.50775</td>\n",
       "      <td>0.50890</td>\n",
       "      <td>0.24765</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.06315</td>\n",
       "      <td>0.04920</td>\n",
       "      <td>0.12335</td>\n",
       "      <td>0.28515</td>\n",
       "      <td>0.33710</td>\n",
       "      <td>0.39055</td>\n",
       "      <td>0.37655</td>\n",
       "      <td>0.42050</td>\n",
       "      <td>0.23555</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.02970</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.13360</td>\n",
       "      <td>0.39940</td>\n",
       "      <td>0.49485</td>\n",
       "      <td>0.52430</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.48935</td>\n",
       "      <td>0.23950</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152806</th>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.04660</td>\n",
       "      <td>0.06660</td>\n",
       "      <td>0.08560</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.26880</td>\n",
       "      <td>0.28690</td>\n",
       "      <td>0.27960</td>\n",
       "      <td>0.28600</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152807</th>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.07690</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.14000</td>\n",
       "      <td>0.25190</td>\n",
       "      <td>0.28530</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>0.33920</td>\n",
       "      <td>0.32180</td>\n",
       "      <td>0.29830</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152808</th>\n",
       "      <td>0.04280</td>\n",
       "      <td>0.04960</td>\n",
       "      <td>0.06970</td>\n",
       "      <td>0.08880</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.18960</td>\n",
       "      <td>0.21990</td>\n",
       "      <td>0.25540</td>\n",
       "      <td>0.25480</td>\n",
       "      <td>0.25390</td>\n",
       "      <td>0.27920</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152809</th>\n",
       "      <td>0.03170</td>\n",
       "      <td>0.04110</td>\n",
       "      <td>0.06480</td>\n",
       "      <td>0.07660</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>0.23340</td>\n",
       "      <td>0.27290</td>\n",
       "      <td>0.30900</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.29640</td>\n",
       "      <td>0.30290</td>\n",
       "      <td>0.1617</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152810</th>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.04140</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.28990</td>\n",
       "      <td>0.33570</td>\n",
       "      <td>0.38040</td>\n",
       "      <td>0.39880</td>\n",
       "      <td>0.40650</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152811 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              B1       B2       B3       B4       B5       B6       B7  \\\n",
       "0        0.00495  0.01885  0.03610  0.05020  0.09065  0.13930  0.15855   \n",
       "1        0.01040  0.02405  0.03980  0.05720  0.10385  0.16755  0.19370   \n",
       "2        0.01510  0.02905  0.06635  0.04515  0.12920  0.38280  0.47410   \n",
       "3        0.01345  0.02925  0.06315  0.04920  0.12335  0.28515  0.33710   \n",
       "4        0.01575  0.02970  0.06900  0.04870  0.13360  0.39940  0.49485   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "1152806  0.03720  0.04660  0.06660  0.08560  0.13300  0.21200  0.24360   \n",
       "1152807  0.03840  0.04250  0.07690  0.08000  0.14000  0.25190  0.28530   \n",
       "1152808  0.04280  0.04960  0.06970  0.08880  0.12820  0.18960  0.21990   \n",
       "1152809  0.03170  0.04110  0.06480  0.07660  0.13350  0.23340  0.27290   \n",
       "1152810  0.03540  0.04140  0.08000  0.07390  0.15220  0.28990  0.33570   \n",
       "\n",
       "              B8      B8A       B9      B11     B12  sample_location_id  class  \n",
       "0        0.18270  0.19205  0.19175  0.20290  0.1097              201701      2  \n",
       "1        0.21600  0.23420  0.24700  0.22290  0.1204              201701      2  \n",
       "2        0.49955  0.50775  0.50890  0.24765  0.1219              201701      2  \n",
       "3        0.39055  0.37655  0.42050  0.23555  0.1175              201701      2  \n",
       "4        0.52430  0.53900  0.48935  0.23950  0.1177              201701      2  \n",
       "...          ...      ...      ...      ...     ...                 ...    ...  \n",
       "1152806  0.26880  0.28690  0.27960  0.28600  0.1640              202312      2  \n",
       "1152807  0.34760  0.33920  0.32180  0.29830  0.1563              202312      2  \n",
       "1152808  0.25540  0.25480  0.25390  0.27920  0.1567              202312      2  \n",
       "1152809  0.30900  0.30940  0.29640  0.30290  0.1617              202312      2  \n",
       "1152810  0.38040  0.39880  0.40650  0.28320  0.1495              202312      2  \n",
       "\n",
       "[1152811 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simply reading the formerly created pickle file, changing it's name to 'merged_df'\n",
    "# To confirm, you can load and view the combined dataset\n",
    "with open(\"combined_dataset.pkl\", 'rb') as file:\n",
    "    merged_df = pd.read_pickle(file)\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>sample_location_id</th>\n",
       "      <th>class</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05020</td>\n",
       "      <td>0.09065</td>\n",
       "      <td>0.13930</td>\n",
       "      <td>0.15855</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.19205</td>\n",
       "      <td>0.19175</td>\n",
       "      <td>0.20290</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.568914</td>\n",
       "      <td>0.246737</td>\n",
       "      <td>-0.052386</td>\n",
       "      <td>0.271183</td>\n",
       "      <td>0.670018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.03980</td>\n",
       "      <td>0.05720</td>\n",
       "      <td>0.10385</td>\n",
       "      <td>0.16755</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.21600</td>\n",
       "      <td>0.23420</td>\n",
       "      <td>0.24700</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.581259</td>\n",
       "      <td>0.287926</td>\n",
       "      <td>-0.015721</td>\n",
       "      <td>0.308070</td>\n",
       "      <td>0.688819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01510</td>\n",
       "      <td>0.02905</td>\n",
       "      <td>0.06635</td>\n",
       "      <td>0.04515</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.38280</td>\n",
       "      <td>0.47410</td>\n",
       "      <td>0.49955</td>\n",
       "      <td>0.50775</td>\n",
       "      <td>0.50890</td>\n",
       "      <td>0.24765</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.834221</td>\n",
       "      <td>0.731688</td>\n",
       "      <td>0.337125</td>\n",
       "      <td>0.652436</td>\n",
       "      <td>0.765506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.06315</td>\n",
       "      <td>0.04920</td>\n",
       "      <td>0.12335</td>\n",
       "      <td>0.28515</td>\n",
       "      <td>0.33710</td>\n",
       "      <td>0.39055</td>\n",
       "      <td>0.37655</td>\n",
       "      <td>0.42050</td>\n",
       "      <td>0.23555</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.776236</td>\n",
       "      <td>0.581962</td>\n",
       "      <td>0.247564</td>\n",
       "      <td>0.544852</td>\n",
       "      <td>0.721622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.02970</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.13360</td>\n",
       "      <td>0.39940</td>\n",
       "      <td>0.49485</td>\n",
       "      <td>0.52430</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.48935</td>\n",
       "      <td>0.23950</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.830017</td>\n",
       "      <td>0.746039</td>\n",
       "      <td>0.372873</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.767403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152806</th>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.04660</td>\n",
       "      <td>0.06660</td>\n",
       "      <td>0.08560</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.26880</td>\n",
       "      <td>0.28690</td>\n",
       "      <td>0.27960</td>\n",
       "      <td>0.28600</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.516930</td>\n",
       "      <td>0.319632</td>\n",
       "      <td>-0.031002</td>\n",
       "      <td>0.321629</td>\n",
       "      <td>0.602862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152807</th>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.07690</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.14000</td>\n",
       "      <td>0.25190</td>\n",
       "      <td>0.28530</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>0.33920</td>\n",
       "      <td>0.32180</td>\n",
       "      <td>0.29830</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625818</td>\n",
       "      <td>0.443384</td>\n",
       "      <td>0.076328</td>\n",
       "      <td>0.432730</td>\n",
       "      <td>0.637691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152808</th>\n",
       "      <td>0.04280</td>\n",
       "      <td>0.04960</td>\n",
       "      <td>0.06970</td>\n",
       "      <td>0.08880</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.18960</td>\n",
       "      <td>0.21990</td>\n",
       "      <td>0.25540</td>\n",
       "      <td>0.25480</td>\n",
       "      <td>0.25390</td>\n",
       "      <td>0.27920</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484021</td>\n",
       "      <td>0.294097</td>\n",
       "      <td>-0.044519</td>\n",
       "      <td>0.296020</td>\n",
       "      <td>0.571209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152809</th>\n",
       "      <td>0.03170</td>\n",
       "      <td>0.04110</td>\n",
       "      <td>0.06480</td>\n",
       "      <td>0.07660</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>0.23340</td>\n",
       "      <td>0.27290</td>\n",
       "      <td>0.30900</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.29640</td>\n",
       "      <td>0.30290</td>\n",
       "      <td>0.1617</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.602697</td>\n",
       "      <td>0.397850</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.393631</td>\n",
       "      <td>0.653291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152810</th>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.04140</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.28990</td>\n",
       "      <td>0.33570</td>\n",
       "      <td>0.38040</td>\n",
       "      <td>0.39880</td>\n",
       "      <td>0.40650</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.674664</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.146474</td>\n",
       "      <td>0.481767</td>\n",
       "      <td>0.652476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152811 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              B1       B2       B3       B4       B5       B6       B7  \\\n",
       "0        0.00495  0.01885  0.03610  0.05020  0.09065  0.13930  0.15855   \n",
       "1        0.01040  0.02405  0.03980  0.05720  0.10385  0.16755  0.19370   \n",
       "2        0.01510  0.02905  0.06635  0.04515  0.12920  0.38280  0.47410   \n",
       "3        0.01345  0.02925  0.06315  0.04920  0.12335  0.28515  0.33710   \n",
       "4        0.01575  0.02970  0.06900  0.04870  0.13360  0.39940  0.49485   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "1152806  0.03720  0.04660  0.06660  0.08560  0.13300  0.21200  0.24360   \n",
       "1152807  0.03840  0.04250  0.07690  0.08000  0.14000  0.25190  0.28530   \n",
       "1152808  0.04280  0.04960  0.06970  0.08880  0.12820  0.18960  0.21990   \n",
       "1152809  0.03170  0.04110  0.06480  0.07660  0.13350  0.23340  0.27290   \n",
       "1152810  0.03540  0.04140  0.08000  0.07390  0.15220  0.28990  0.33570   \n",
       "\n",
       "              B8      B8A       B9      B11     B12  sample_location_id  \\\n",
       "0        0.18270  0.19205  0.19175  0.20290  0.1097              201701   \n",
       "1        0.21600  0.23420  0.24700  0.22290  0.1204              201701   \n",
       "2        0.49955  0.50775  0.50890  0.24765  0.1219              201701   \n",
       "3        0.39055  0.37655  0.42050  0.23555  0.1175              201701   \n",
       "4        0.52430  0.53900  0.48935  0.23950  0.1177              201701   \n",
       "...          ...      ...      ...      ...     ...                 ...   \n",
       "1152806  0.26880  0.28690  0.27960  0.28600  0.1640              202312   \n",
       "1152807  0.34760  0.33920  0.32180  0.29830  0.1563              202312   \n",
       "1152808  0.25540  0.25480  0.25390  0.27920  0.1567              202312   \n",
       "1152809  0.30900  0.30940  0.29640  0.30290  0.1617              202312   \n",
       "1152810  0.38040  0.39880  0.40650  0.28320  0.1495              202312   \n",
       "\n",
       "         class      NDVI       EVI      NDWI      SAVI     GNDVI  \n",
       "0            2  0.568914  0.246737 -0.052386  0.271183  0.670018  \n",
       "1            2  0.581259  0.287926 -0.015721  0.308070  0.688819  \n",
       "2            2  0.834221  0.731688  0.337125  0.652436  0.765506  \n",
       "3            2  0.776236  0.581962  0.247564  0.544852  0.721622  \n",
       "4            2  0.830017  0.746039  0.372873  0.664865  0.767403  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "1152806      2  0.516930  0.319632 -0.031002  0.321629  0.602862  \n",
       "1152807      2  0.625818  0.443384  0.076328  0.432730  0.637691  \n",
       "1152808      2  0.484021  0.294097 -0.044519  0.296020  0.571209  \n",
       "1152809      2  0.602697  0.397850  0.009969  0.393631  0.653291  \n",
       "1152810      2  0.674664  0.506344  0.146474  0.481767  0.652476  \n",
       "\n",
       "[1152811 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['NDVI'] = (merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + merged_df['B4'])\n",
    "\n",
    "# EVI is an optimized vegetation index designed to enhance the vegetation signal with improved sensitivity in high biomass regions\n",
    "# It's calculated using the Red (B4), Near-Infrared (B8 or B5), and Blue (B2) bands.\n",
    "merged_df['EVI'] = 2.5 * (merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + 6 * merged_df['B4'] - 7.5 * merged_df['B2'] + 1)\n",
    "\n",
    "# NDWI is used to monitor changes in water content of leaves\n",
    "# It is typically calculated using the Near-Infrared (B8 or B5) and Short-Wave Infrared (B11 or B6) bands.\n",
    "merged_df['NDWI'] = (merged_df['B8'] - merged_df['B11']) / (merged_df['B8'] + merged_df['B11'])\n",
    "\n",
    "# SAVI is a modification of NDVI to correct for the influence of soil brightness\n",
    "# The standard value of L in the SAVI formula is 0.5.\n",
    "L = 0.5  # soil brightness correction factor\n",
    "merged_df['SAVI'] = ((merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + merged_df['B4'] + L)) * (1 + L)\n",
    "\n",
    "# GNDVI is used to estimate vegetation health\n",
    "# It's calculated using the Near-Infrared (B8 or B5) and Green (B3) bands.\n",
    "merged_df['GNDVI'] = (merged_df['B8'] - merged_df['B3']) / (merged_df['B8'] + merged_df['B3'])\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "merged_df.to_csv('merged_df.csv', index=True)  # Adjust the filename as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collapsing locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'merged_df' is your pandas DataFrame\n",
    "merged_df = merged_df.drop('sample_location_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Constructing stratified K-fold cross validation (SKF)\n",
    "\n",
    "Stratified just means you're trying to ensure each fold has the same relative proportion of each class so that you're training and testing on the same proportion of data for each K-fold. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the cell below is to find the optimal number of folds which are either individual locations or groupings of individual locations so that the relative class proportions across all folds are as balanced as possible\n",
    "\n",
    "It's meant to save you from staring at the class distributions per location and trying to manually come up with the folds yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming 'class' column to 'classes' to fix python error\n",
    "merged_df = merged_df.rename(columns={'class': 'classes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = actively eroding,\n",
    "1 = drained, \n",
    "2 = modified, \n",
    "3 = bare peat, \n",
    "4 = restored, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>classes</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.04940</td>\n",
       "      <td>0.07090</td>\n",
       "      <td>0.08650</td>\n",
       "      <td>0.13200</td>\n",
       "      <td>0.21270</td>\n",
       "      <td>0.24250</td>\n",
       "      <td>0.27180</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>0.27020</td>\n",
       "      <td>0.31170</td>\n",
       "      <td>0.17690</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517164</td>\n",
       "      <td>0.326164</td>\n",
       "      <td>-0.068380</td>\n",
       "      <td>0.323838</td>\n",
       "      <td>0.586227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.02810</td>\n",
       "      <td>0.06120</td>\n",
       "      <td>0.03640</td>\n",
       "      <td>0.10860</td>\n",
       "      <td>0.35290</td>\n",
       "      <td>0.45100</td>\n",
       "      <td>0.47480</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.46610</td>\n",
       "      <td>0.23810</td>\n",
       "      <td>0.10850</td>\n",
       "      <td>4</td>\n",
       "      <td>0.857590</td>\n",
       "      <td>0.739317</td>\n",
       "      <td>0.332024</td>\n",
       "      <td>0.650316</td>\n",
       "      <td>0.771642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.05670</td>\n",
       "      <td>0.07940</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>0.14685</td>\n",
       "      <td>0.21885</td>\n",
       "      <td>0.25335</td>\n",
       "      <td>0.28550</td>\n",
       "      <td>0.2913</td>\n",
       "      <td>0.29245</td>\n",
       "      <td>0.30345</td>\n",
       "      <td>0.18250</td>\n",
       "      <td>1</td>\n",
       "      <td>0.477743</td>\n",
       "      <td>0.314877</td>\n",
       "      <td>-0.030478</td>\n",
       "      <td>0.312387</td>\n",
       "      <td>0.564812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.06020</td>\n",
       "      <td>0.06860</td>\n",
       "      <td>0.12090</td>\n",
       "      <td>0.26350</td>\n",
       "      <td>0.29060</td>\n",
       "      <td>0.31320</td>\n",
       "      <td>0.3408</td>\n",
       "      <td>0.33520</td>\n",
       "      <td>0.24780</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>4</td>\n",
       "      <td>0.640650</td>\n",
       "      <td>0.427607</td>\n",
       "      <td>0.116578</td>\n",
       "      <td>0.416081</td>\n",
       "      <td>0.677558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.03150</td>\n",
       "      <td>0.05255</td>\n",
       "      <td>0.05850</td>\n",
       "      <td>0.10105</td>\n",
       "      <td>0.18555</td>\n",
       "      <td>0.21140</td>\n",
       "      <td>0.26230</td>\n",
       "      <td>0.2496</td>\n",
       "      <td>0.26135</td>\n",
       "      <td>0.23120</td>\n",
       "      <td>0.14775</td>\n",
       "      <td>3</td>\n",
       "      <td>0.635287</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>0.063019</td>\n",
       "      <td>0.372442</td>\n",
       "      <td>0.666190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152805</th>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.03410</td>\n",
       "      <td>0.04840</td>\n",
       "      <td>0.05670</td>\n",
       "      <td>0.10300</td>\n",
       "      <td>0.18860</td>\n",
       "      <td>0.21890</td>\n",
       "      <td>0.23350</td>\n",
       "      <td>0.2488</td>\n",
       "      <td>0.25060</td>\n",
       "      <td>0.22140</td>\n",
       "      <td>0.11940</td>\n",
       "      <td>1</td>\n",
       "      <td>0.609235</td>\n",
       "      <td>0.335369</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>0.335611</td>\n",
       "      <td>0.656616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152806</th>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.03210</td>\n",
       "      <td>0.04950</td>\n",
       "      <td>0.04860</td>\n",
       "      <td>0.09730</td>\n",
       "      <td>0.20170</td>\n",
       "      <td>0.24280</td>\n",
       "      <td>0.27270</td>\n",
       "      <td>0.2738</td>\n",
       "      <td>0.29780</td>\n",
       "      <td>0.18320</td>\n",
       "      <td>0.08830</td>\n",
       "      <td>1</td>\n",
       "      <td>0.697479</td>\n",
       "      <td>0.423293</td>\n",
       "      <td>0.196315</td>\n",
       "      <td>0.409290</td>\n",
       "      <td>0.692737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152807</th>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.03675</td>\n",
       "      <td>0.05535</td>\n",
       "      <td>0.06445</td>\n",
       "      <td>0.13925</td>\n",
       "      <td>0.19975</td>\n",
       "      <td>0.22960</td>\n",
       "      <td>0.23735</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.26345</td>\n",
       "      <td>0.26630</td>\n",
       "      <td>0.15395</td>\n",
       "      <td>2</td>\n",
       "      <td>0.572896</td>\n",
       "      <td>0.320559</td>\n",
       "      <td>-0.057480</td>\n",
       "      <td>0.323460</td>\n",
       "      <td>0.621797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152808</th>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.03150</td>\n",
       "      <td>0.05790</td>\n",
       "      <td>0.05620</td>\n",
       "      <td>0.11530</td>\n",
       "      <td>0.22010</td>\n",
       "      <td>0.25380</td>\n",
       "      <td>0.29080</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.31660</td>\n",
       "      <td>0.23070</td>\n",
       "      <td>0.11430</td>\n",
       "      <td>1</td>\n",
       "      <td>0.676081</td>\n",
       "      <td>0.421412</td>\n",
       "      <td>0.115245</td>\n",
       "      <td>0.415466</td>\n",
       "      <td>0.667909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152809</th>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.03250</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.05940</td>\n",
       "      <td>0.10690</td>\n",
       "      <td>0.20410</td>\n",
       "      <td>0.23610</td>\n",
       "      <td>0.24980</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.13130</td>\n",
       "      <td>2</td>\n",
       "      <td>0.615783</td>\n",
       "      <td>0.349371</td>\n",
       "      <td>0.029254</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.673702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152810 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             B1       B2       B3       B4       B5       B6       B7  \\\n",
       "0        0.0314  0.04940  0.07090  0.08650  0.13200  0.21270  0.24250   \n",
       "1        0.0276  0.02810  0.06120  0.03640  0.10860  0.35290  0.45100   \n",
       "2        0.0430  0.05670  0.07940  0.10090  0.14685  0.21885  0.25335   \n",
       "3        0.0383  0.03930  0.06020  0.06860  0.12090  0.26350  0.29060   \n",
       "4        0.0186  0.03150  0.05255  0.05850  0.10105  0.18555  0.21140   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "1152805  0.0294  0.03410  0.04840  0.05670  0.10300  0.18860  0.21890   \n",
       "1152806  0.0273  0.03210  0.04950  0.04860  0.09730  0.20170  0.24280   \n",
       "1152807  0.0343  0.03675  0.05535  0.06445  0.13925  0.19975  0.22960   \n",
       "1152808  0.0319  0.03150  0.05790  0.05620  0.11530  0.22010  0.25380   \n",
       "1152809  0.0311  0.03250  0.04870  0.05940  0.10690  0.20410  0.23610   \n",
       "\n",
       "              B8     B8A       B9      B11      B12  classes      NDVI  \\\n",
       "0        0.27180  0.2772  0.27020  0.31170  0.17690        1  0.517164   \n",
       "1        0.47480  0.4947  0.46610  0.23810  0.10850        4  0.857590   \n",
       "2        0.28550  0.2913  0.29245  0.30345  0.18250        1  0.477743   \n",
       "3        0.31320  0.3408  0.33520  0.24780  0.13710        4  0.640650   \n",
       "4        0.26230  0.2496  0.26135  0.23120  0.14775        3  0.635287   \n",
       "...          ...     ...      ...      ...      ...      ...       ...   \n",
       "1152805  0.23350  0.2488  0.25060  0.22140  0.11940        1  0.609235   \n",
       "1152806  0.27270  0.2738  0.29780  0.18320  0.08830        1  0.697479   \n",
       "1152807  0.23735  0.2646  0.26345  0.26630  0.15395        2  0.572896   \n",
       "1152808  0.29080  0.2940  0.31660  0.23070  0.11430        1  0.676081   \n",
       "1152809  0.24980  0.2700  0.25170  0.23560  0.13130        2  0.615783   \n",
       "\n",
       "              EVI      NDWI      SAVI     GNDVI  \n",
       "0        0.326164 -0.068380  0.323838  0.586227  \n",
       "1        0.739317  0.332024  0.650316  0.771642  \n",
       "2        0.314877 -0.030478  0.312387  0.564812  \n",
       "3        0.427607  0.116578  0.416081  0.677558  \n",
       "4        0.369994  0.063019  0.372442  0.666190  \n",
       "...           ...       ...       ...       ...  \n",
       "1152805  0.335369  0.026599  0.335611  0.656616  \n",
       "1152806  0.423293  0.196315  0.409290  0.692737  \n",
       "1152807  0.320559 -0.057480  0.323460  0.621797  \n",
       "1152808  0.421412  0.115245  0.415466  0.667909  \n",
       "1152809  0.349371  0.029254  0.352941  0.673702  \n",
       "\n",
       "[1152810 rows x 18 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming merged_df is your pandas DataFrame\n",
    "# Shuffle the DataFrame using a random seed, for example, seed=42\n",
    "shuffled_df = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "shuffled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. constructing training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your features are all columns except 'classes', and 'classes' is the target variable\n",
    "X = shuffled_df.drop('classes', axis=1)  # Features\n",
    "y = shuffled_df['classes']  # Target variable\n",
    "\n",
    "# Perform the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# X_train and y_train will now contain 70% of the data, X_test and y_test will contain 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X_train to a CSV file\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "\n",
    "# Save X_test to a CSV file\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "\n",
    "# Save y_train to a CSV file\n",
    "# Since y_train is a Series, we convert it to a DataFrame for consistency\n",
    "y_train.to_frame().to_csv('y_train.csv', index=False)\n",
    "\n",
    "# Save y_test to a CSV file\n",
    "# Since y_test is a Series, we convert it to a DataFrame for consistency\n",
    "y_test.to_frame().to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna on train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 400)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1.0)\n",
    "    min_samples_leaf = trial.suggest_float('min_samples_leaf', 0.1, 0.5)\n",
    "    \n",
    "    \n",
    "    # Initialize the classifier with the current hyperparameters\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    # Compute and return the accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-11 22:44:11,774] A new study created in memory with name: no-name-80c97423-6fb3-4a7f-8e8e-211730445a2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-11 22:45:01,029] Trial 0 finished with value: 0.4287870507715929 and parameters: {'n_estimators': 291, 'max_depth': 2, 'min_samples_split': 0.4633394932831646, 'min_samples_leaf': 0.41898194416292645}. Best is trial 0 with value: 0.4287870507715929.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. training / evaluating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a baseline model, the simplest possible model, using logistic regression on the SKF you sorted out above. This establishes your baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found out that there were NaN values in dataset, but only three, so I first remove them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any NaN values in the DataFrame? True\n"
     ]
    }
   ],
   "source": [
    "# Check for any NaN values in the DataFrame\n",
    "nan_exists = merged_df.isnull().values.any()\n",
    "\n",
    "# Print result\n",
    "print(f\"Are there any NaN values in the DataFrame? {nan_exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI     1\n",
      "NDWI     1\n",
      "GNDVI    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the count of NaNs in each column\n",
    "nan_counts = merged_df.isnull().sum()\n",
    "\n",
    "# Print columns with NaN counts greater than 0\n",
    "print(nan_counts[nan_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy across all folds: 0.4983813464491113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Prepare features (X) and target (y)\n",
    "features = merged_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']].values\n",
    "target = merged_df['classes'].values\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)  # best_k determined from your previous code\n",
    "\n",
    "accuracy_scores = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for train_index, test_index in skf.split(features, target):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "    # Scale the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train Logistic Regression Model with increased max_iter and adjusted C\n",
    "    model = LogisticRegression(max_iter=5000, C=10)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the Model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "print(f\"Average Accuracy across all folds: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some additional assessments as to how it performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.49395725555568754\n",
      "Recall: 0.4069454829655209\n",
      "F1 Score: 0.3931784864288107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_test and y_pred are already defined from your previous code\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred, average='macro')  # 'macro' averages over classes\n",
    "recall = recall_score(y_test, y_pred, average='macro')  # 'macro' averages over classes\n",
    "f1 = f1_score(y_test, y_pred, average='macro')  # 'macro' averages over classes\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly an RF model across all features, that is, bands and VIs inclusive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Bands and VIs Average Accuracy: 0.7909941794398037\n",
      "RF Bands and VIs Average Precision: 0.8228059357534965\n",
      "RF Bands and VIs Average Recall: 0.7477214027472262\n",
      "RF Bands and VIs Average F1 Score: 0.7767133766215912\n"
     ]
    }
   ],
   "source": [
    "# features = merged_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']].values\n",
    "# target = merged_df['classes'].values\n",
    "\n",
    "# # Assuming features and target are already defined\n",
    "# skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)\n",
    "\n",
    "# rf_all_accuracy_scores = []\n",
    "# rf_all_precision_scores = []\n",
    "# rf_all_recall_scores = []\n",
    "# rf_all_f1_scores = []\n",
    "\n",
    "# for train_index, test_index in skf.split(features, target):\n",
    "#     rf_all_X_train, rf_all_X_test = features[train_index], features[test_index]\n",
    "#     rf_all_y_train, rf_all_y_test = target[train_index], target[test_index]\n",
    "\n",
    "#     rf_all_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     rf_all_model.fit(rf_all_X_train, rf_all_y_train)\n",
    "\n",
    "#     rf_all_y_pred = rf_all_model.predict(rf_all_X_test)\n",
    "\n",
    "#     rf_all_accuracy_scores.append(accuracy_score(rf_all_y_test, rf_all_y_pred))\n",
    "#     rf_all_precision_scores.append(precision_score(rf_all_y_test, rf_all_y_pred, average='macro'))\n",
    "#     rf_all_recall_scores.append(recall_score(rf_all_y_test, rf_all_y_pred, average='macro'))\n",
    "#     rf_all_f1_scores.append(f1_score(rf_all_y_test, rf_all_y_pred, average='macro'))\n",
    "\n",
    "# # Calculate the average scores across all folds\n",
    "# rf_all_average_accuracy = np.mean(rf_all_accuracy_scores)\n",
    "# rf_all_average_precision = np.mean(rf_all_precision_scores)\n",
    "# rf_all_average_recall = np.mean(rf_all_recall_scores)\n",
    "# rf_all_average_f1 = np.mean(rf_all_f1_scores)\n",
    "\n",
    "# print(f\"RF Bands and VIs Average Accuracy: {rf_all_average_accuracy}\")\n",
    "# print(f\"RF Bands and VIs Average Precision: {rf_all_average_precision}\")\n",
    "# print(f\"RF Bands and VIs Average Recall: {rf_all_average_recall}\")\n",
    "# print(f\"RF Bands and VIs Average F1 Score: {rf_all_average_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance for RF all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: [0.10656363 0.06256748 0.04295264 0.05107344 0.06812389 0.05333279\n",
      " 0.05057216 0.03411956 0.04960241 0.08781716 0.07903299 0.08170818\n",
      " 0.04654216 0.04809754 0.04860802 0.04090257 0.0483834 ]\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train and y_train are your training data and labels\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(rf_all_X_train, rf_all_y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature Importances:\", importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: 0.10656363173606073\n",
      "B9: 0.0878171613488721\n",
      "B12: 0.08170817956227323\n",
      "B11: 0.0790329869247652\n",
      "B5: 0.06812388761845203\n",
      "B2: 0.06256747539785273\n",
      "B6: 0.053332786586987375\n",
      "B4: 0.051073438109255485\n",
      "B7: 0.05057215880142362\n",
      "B8A: 0.04960240645138941\n",
      "NDWI: 0.04860801975760083\n",
      "GNDVI: 0.04838339825625264\n",
      "EVI: 0.048097543857085985\n",
      "NDVI: 0.046542156064613066\n",
      "B3: 0.042952637654631226\n",
      "SAVI: 0.04090256894677449\n",
      "B8: 0.034119562925709966\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a list of feature names corresponding to the order of features provided to the model\n",
    "feature_names = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']\n",
    "\n",
    "# Mapping feature importance scores to their corresponding feature names\n",
    "importances = clf.feature_importances_\n",
    "feature_importance_dict = dict(zip(feature_names, importances))\n",
    "\n",
    "# Printing feature importances in a more interpretable way\n",
    "for feature, importance in sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance for RF bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bands Feature Importances: [0.12299149 0.07357307 0.05953855 0.07845013 0.08501202 0.07200014\n",
      " 0.06934583 0.05797456 0.06859535 0.10872497 0.1013323  0.10246157]\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train and y_train are your training data and labels\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(rf_bands_X_train, rf_bands_y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importances_bands = clf.feature_importances_\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Bands Feature Importances:\", importances_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: 0.1229914859884394\n",
      "B9: 0.10872496941456683\n",
      "B12: 0.10246157464367561\n",
      "B11: 0.10133230009645854\n",
      "B5: 0.08501202469588465\n",
      "B4: 0.07845012851143043\n",
      "B2: 0.07357307346374911\n",
      "B6: 0.07200014413815138\n",
      "B7: 0.06934583148255291\n",
      "B8A: 0.06859535119362262\n",
      "B3: 0.05953855188359311\n",
      "B8: 0.05797456448787542\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a list of feature names corresponding to the order of features provided to the model\n",
    "feature_names = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
    "\n",
    "# Mapping feature importance scores to their corresponding feature names\n",
    "importances_bands = clf.feature_importances_\n",
    "feature_importance_dict = dict(zip(feature_names, importances_bands))\n",
    "\n",
    "# Printing feature importances in a more interpretable way\n",
    "for feature, importance in sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to find the best hyperparameter setting for your models:\n",
    "\n",
    "you could use something like OPtuna and do it below, using k-fold cross validation, using the k and location splits above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-06 16:30:34,756] A new study created in memory with name: no-name-ec237593-d786-459a-9102-acc553e9c459\n",
      "[I 2024-02-06 16:43:54,766] Trial 0 finished with value: 0.5072605199469123 and parameters: {'n_estimators': 159, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 16}. Best is trial 0 with value: 0.5072605199469123.\n",
      "[I 2024-02-06 17:03:12,775] Trial 1 finished with value: 0.49176967583556697 and parameters: {'n_estimators': 255, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 11}. Best is trial 0 with value: 0.5072605199469123.\n",
      "[I 2024-02-06 17:05:15,888] Trial 2 finished with value: 0.4792107979632376 and parameters: {'n_estimators': 44, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.5072605199469123.\n",
      "[I 2024-02-06 17:28:50,791] Trial 3 finished with value: 0.5445893078651296 and parameters: {'n_estimators': 194, 'max_depth': 9, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 3 with value: 0.5445893078651296.\n",
      "[I 2024-02-06 17:34:25,336] Trial 4 finished with value: 0.542740781221537 and parameters: {'n_estimators': 43, 'max_depth': 9, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.5445893078651296.\n",
      "[I 2024-02-06 17:49:38,067] Trial 5 finished with value: 0.6056201802552025 and parameters: {'n_estimators': 82, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.6056201802552025.\n",
      "[I 2024-02-06 17:51:25,883] Trial 6 finished with value: 0.47489872572236536 and parameters: {'n_estimators': 61, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 5 with value: 0.6056201802552025.\n",
      "[I 2024-02-06 18:06:47,782] Trial 7 finished with value: 0.683802187697886 and parameters: {'n_estimators': 59, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 7 with value: 0.683802187697886.\n",
      "[I 2024-02-06 18:10:58,105] Trial 8 finished with value: 0.473429272820326 and parameters: {'n_estimators': 128, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 7 with value: 0.683802187697886.\n",
      "[I 2024-02-06 18:23:52,181] Trial 9 finished with value: 0.47906593454255253 and parameters: {'n_estimators': 298, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 11}. Best is trial 7 with value: 0.683802187697886.\n",
      "[I 2024-02-06 18:55:32,107] Trial 10 finished with value: 0.7332283724117592 and parameters: {'n_estimators': 13, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 10 with value: 0.7332283724117592.\n",
      "[I 2024-02-06 19:03:48,373] Trial 11 finished with value: 0.744597982321458 and parameters: {'n_estimators': 26, 'max_depth': 32, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.744597982321458.\n",
      "[I 2024-02-06 19:09:28,452] Trial 12 finished with value: 0.7256685837215153 and parameters: {'n_estimators': 13, 'max_depth': 27, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.744597982321458.\n",
      "[I 2024-02-06 20:44:03,040] Trial 13 finished with value: 0.7576816648016585 and parameters: {'n_estimators': 111, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-06 21:22:57,669] Trial 14 finished with value: 0.7094456154960488 and parameters: {'n_estimators': 113, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-06 22:02:52,956] Trial 15 finished with value: 0.738924887882652 and parameters: {'n_estimators': 195, 'max_depth': 32, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-06 23:54:57,892] Trial 16 finished with value: 0.6859855483557569 and parameters: {'n_estimators': 104, 'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 10:01:28,940] Trial 17 finished with value: 0.7102193770005465 and parameters: {'n_estimators': 159, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 15}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 10:39:17,412] Trial 18 finished with value: 0.6246588770048838 and parameters: {'n_estimators': 201, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 11:04:01,415] Trial 19 finished with value: 0.7096433931003374 and parameters: {'n_estimators': 84, 'max_depth': 22, 'min_samples_split': 13, 'min_samples_leaf': 13}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 11:40:39,150] Trial 20 finished with value: 0.6386568471821028 and parameters: {'n_estimators': 130, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 12:56:15,170] Trial 21 finished with value: 0.7387540010929815 and parameters: {'n_estimators': 200, 'max_depth': 31, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 14:43:47,476] Trial 22 finished with value: 0.7390671489664384 and parameters: {'n_estimators': 237, 'max_depth': 32, 'min_samples_split': 3, 'min_samples_leaf': 8}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 16:06:18,252] Trial 23 finished with value: 0.7283212324667552 and parameters: {'n_estimators': 259, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 17:00:17,028] Trial 24 finished with value: 0.7274338355843548 and parameters: {'n_estimators': 239, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 13 with value: 0.7576816648016585.\n",
      "[I 2024-02-07 18:45:39,814] Trial 25 finished with value: 0.7610013792385562 and parameters: {'n_estimators': 288, 'max_depth': 32, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-07 21:48:52,969] Trial 26 finished with value: 0.7138878045818479 and parameters: {'n_estimators': 281, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-07 22:57:54,739] Trial 27 finished with value: 0.5677848040874038 and parameters: {'n_estimators': 144, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-07 23:09:15,989] Trial 28 finished with value: 0.4963732098090752 and parameters: {'n_estimators': 91, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-07 23:33:06,899] Trial 29 finished with value: 0.657438780024462 and parameters: {'n_estimators': 168, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 00:11:28,294] Trial 30 finished with value: 0.7492743817281252 and parameters: {'n_estimators': 176, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 01:06:06,147] Trial 31 finished with value: 0.7531830917497246 and parameters: {'n_estimators': 215, 'max_depth': 26, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 01:49:55,502] Trial 32 finished with value: 0.7493290308029944 and parameters: {'n_estimators': 177, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 02:36:46,452] Trial 33 finished with value: 0.7148307179847503 and parameters: {'n_estimators': 220, 'max_depth': 20, 'min_samples_split': 9, 'min_samples_leaf': 6}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 03:45:46,249] Trial 34 finished with value: 0.7585014009246971 and parameters: {'n_estimators': 270, 'max_depth': 25, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 04:26:35,021] Trial 35 finished with value: 0.5885401757444852 and parameters: {'n_estimators': 272, 'max_depth': 11, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 04:46:04,040] Trial 36 finished with value: 0.48330340645900016 and parameters: {'n_estimators': 290, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 1}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 05:15:39,247] Trial 37 finished with value: 0.5084385111163158 and parameters: {'n_estimators': 261, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 06:19:51,541] Trial 38 finished with value: 0.6857070983076136 and parameters: {'n_estimators': 234, 'max_depth': 17, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 25 with value: 0.7610013792385562.\n",
      "[I 2024-02-08 07:01:27,318] Trial 39 finished with value: 0.7647669607307361 and parameters: {'n_estimators': 218, 'max_depth': 27, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 39 with value: 0.7647669607307361.\n",
      "[I 2024-02-08 07:33:01,580] Trial 40 finished with value: 0.6718678706812051 and parameters: {'n_estimators': 273, 'max_depth': 16, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 39 with value: 0.7647669607307361.\n",
      "[I 2024-02-08 08:15:24,515] Trial 41 finished with value: 0.7536723310866491 and parameters: {'n_estimators': 248, 'max_depth': 27, 'min_samples_split': 16, 'min_samples_leaf': 1}. Best is trial 39 with value: 0.7647669607307361.\n",
      "[I 2024-02-08 08:58:46,853] Trial 42 finished with value: 0.7584155238070454 and parameters: {'n_estimators': 247, 'max_depth': 28, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 39 with value: 0.7647669607307361.\n",
      "[I 2024-02-08 09:30:26,507] Trial 43 finished with value: 0.7172040492362142 and parameters: {'n_estimators': 221, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 39 with value: 0.7647669607307361.\n",
      "[I 2024-02-08 10:22:03,672] Trial 44 finished with value: 0.7651737927325405 and parameters: {'n_estimators': 293, 'max_depth': 27, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-08 11:58:09,380] Trial 45 finished with value: 0.7401748770395815 and parameters: {'n_estimators': 300, 'max_depth': 22, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-08 12:36:51,050] Trial 46 finished with value: 0.5250067227036546 and parameters: {'n_estimators': 284, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-08 14:56:03,895] Trial 47 finished with value: 0.7599066628499058 and parameters: {'n_estimators': 268, 'max_depth': 28, 'min_samples_split': 11, 'min_samples_leaf': 2}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-08 16:04:11,092] Trial 48 finished with value: 0.7090778185477224 and parameters: {'n_estimators': 272, 'max_depth': 19, 'min_samples_split': 11, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-08 16:13:58,935] Trial 49 finished with value: 0.47920559328944057 and parameters: {'n_estimators': 263, 'max_depth': 3, 'min_samples_split': 11, 'min_samples_leaf': 2}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-08 18:21:14,343] Trial 50 finished with value: 0.7601304638231798 and parameters: {'n_estimators': 288, 'max_depth': 29, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-08 20:41:57,131] Trial 51 finished with value: 0.7601304638231798 and parameters: {'n_estimators': 288, 'max_depth': 29, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 00:00:39,849] Trial 52 finished with value: 0.7570588388372759 and parameters: {'n_estimators': 291, 'max_depth': 29, 'min_samples_split': 12, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 01:31:15,459] Trial 53 finished with value: 0.7599925399675576 and parameters: {'n_estimators': 299, 'max_depth': 29, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 02:33:56,114] Trial 54 finished with value: 0.7312558010426696 and parameters: {'n_estimators': 299, 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 03:41:23,195] Trial 55 finished with value: 0.7621672261690998 and parameters: {'n_estimators': 283, 'max_depth': 32, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 04:37:59,443] Trial 56 finished with value: 0.7145635447298341 and parameters: {'n_estimators': 284, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 16}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 05:31:42,743] Trial 57 finished with value: 0.7417085209184515 and parameters: {'n_estimators': 248, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 05:44:04,098] Trial 58 finished with value: 0.4833589229795022 and parameters: {'n_estimators': 280, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 06:34:17,430] Trial 59 finished with value: 0.7220036259227454 and parameters: {'n_estimators': 258, 'max_depth': 32, 'min_samples_split': 10, 'min_samples_leaf': 13}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 07:14:14,186] Trial 60 finished with value: 0.7220895030403969 and parameters: {'n_estimators': 230, 'max_depth': 21, 'min_samples_split': 13, 'min_samples_leaf': 6}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 08:10:45,197] Trial 61 finished with value: 0.7575324641528092 and parameters: {'n_estimators': 291, 'max_depth': 29, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 09:05:35,550] Trial 62 finished with value: 0.7567127280297707 and parameters: {'n_estimators': 300, 'max_depth': 27, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 09:44:23,086] Trial 63 finished with value: 0.6991030612156384 and parameters: {'n_estimators': 280, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 10:34:24,087] Trial 64 finished with value: 0.7572392675289077 and parameters: {'n_estimators': 293, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 11:12:31,798] Trial 65 finished with value: 0.729994535092513 and parameters: {'n_estimators': 251, 'max_depth': 23, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 12:26:22,772] Trial 66 finished with value: 0.7482100259366244 and parameters: {'n_estimators': 283, 'max_depth': 26, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 14:03:23,555] Trial 67 finished with value: 0.7649612685524935 and parameters: {'n_estimators': 276, 'max_depth': 32, 'min_samples_split': 11, 'min_samples_leaf': 2}. Best is trial 44 with value: 0.7651737927325405.\n",
      "[I 2024-02-09 15:46:06,032] Trial 68 finished with value: 0.7672747460552909 and parameters: {'n_estimators': 276, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 17:06:41,455] Trial 69 finished with value: 0.7669208282370903 and parameters: {'n_estimators': 211, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 18:51:34,750] Trial 70 finished with value: 0.7638474683599206 and parameters: {'n_estimators': 188, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 19:56:42,092] Trial 71 finished with value: 0.7638075658608097 and parameters: {'n_estimators': 183, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 20:46:22,689] Trial 72 finished with value: 0.7470693349294333 and parameters: {'n_estimators': 187, 'max_depth': 24, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 21:38:12,706] Trial 73 finished with value: 0.7509659007121728 and parameters: {'n_estimators': 211, 'max_depth': 26, 'min_samples_split': 14, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 22:36:31,791] Trial 74 finished with value: 0.7661939087967662 and parameters: {'n_estimators': 153, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 23:08:05,931] Trial 75 finished with value: 0.7524709188851588 and parameters: {'n_estimators': 147, 'max_depth': 25, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-09 23:41:22,194] Trial 76 finished with value: 0.7638578777075147 and parameters: {'n_estimators': 187, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 00:08:44,728] Trial 77 finished with value: 0.7554557993077784 and parameters: {'n_estimators': 164, 'max_depth': 27, 'min_samples_split': 12, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 00:38:28,609] Trial 78 finished with value: 0.7268717308142711 and parameters: {'n_estimators': 204, 'max_depth': 21, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 00:46:44,184] Trial 79 finished with value: 0.4958076352564603 and parameters: {'n_estimators': 191, 'max_depth': 6, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 01:07:34,939] Trial 80 finished with value: 0.7370876380322864 and parameters: {'n_estimators': 133, 'max_depth': 23, 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 01:40:54,038] Trial 81 finished with value: 0.7638075658608097 and parameters: {'n_estimators': 183, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 02:08:17,255] Trial 82 finished with value: 0.7616207354204074 and parameters: {'n_estimators': 151, 'max_depth': 30, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 02:38:02,859] Trial 83 finished with value: 0.755687407291748 and parameters: {'n_estimators': 172, 'max_depth': 27, 'min_samples_split': 12, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 03:13:27,447] Trial 84 finished with value: 0.7591320338997753 and parameters: {'n_estimators': 198, 'max_depth': 30, 'min_samples_split': 15, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 03:39:13,162] Trial 85 finished with value: 0.7431224572999888 and parameters: {'n_estimators': 158, 'max_depth': 24, 'min_samples_split': 14, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 04:16:02,609] Trial 86 finished with value: 0.7607949271779391 and parameters: {'n_estimators': 209, 'max_depth': 27, 'min_samples_split': 11, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 04:57:33,084] Trial 87 finished with value: 0.7641883744936286 and parameters: {'n_estimators': 227, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 05:37:53,558] Trial 88 finished with value: 0.7608296250032529 and parameters: {'n_estimators': 228, 'max_depth': 30, 'min_samples_split': 12, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 06:16:06,631] Trial 89 finished with value: 0.7545952932399962 and parameters: {'n_estimators': 223, 'max_depth': 25, 'min_samples_split': 11, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 06:36:37,622] Trial 90 finished with value: 0.7188591355036823 and parameters: {'n_estimators': 141, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 07:09:38,687] Trial 91 finished with value: 0.7638084333064425 and parameters: {'n_estimators': 179, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 07:52:14,619] Trial 92 finished with value: 0.7583391885913551 and parameters: {'n_estimators': 240, 'max_depth': 28, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 08:28:35,855] Trial 93 finished with value: 0.7592378622669824 and parameters: {'n_estimators': 205, 'max_depth': 30, 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 09:02:04,682] Trial 94 finished with value: 0.7541693774342693 and parameters: {'n_estimators': 192, 'max_depth': 26, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 09:35:18,855] Trial 95 finished with value: 0.7278554141619173 and parameters: {'n_estimators': 216, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 11}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 10:07:06,568] Trial 96 finished with value: 0.7631179465826979 and parameters: {'n_estimators': 179, 'max_depth': 28, 'min_samples_split': 11, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 10:32:47,036] Trial 97 finished with value: 0.7382456779521344 and parameters: {'n_estimators': 165, 'max_depth': 23, 'min_samples_split': 14, 'min_samples_leaf': 2}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 11:02:48,459] Trial 98 finished with value: 0.761791622210078 and parameters: {'n_estimators': 170, 'max_depth': 30, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 68 with value: 0.7672747460552909.\n",
      "[I 2024-02-10 11:20:50,238] Trial 99 finished with value: 0.6223063644486082 and parameters: {'n_estimators': 196, 'max_depth': 13, 'min_samples_split': 12, 'min_samples_leaf': 10}. Best is trial 68 with value: 0.7672747460552909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'n_estimators': 276, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = merged_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']].values\n",
    "target = merged_df['classes'].values\n",
    "\n",
    "# Assuming 'best_k' is defined based on your previous analysis\n",
    "# Assuming 'features' and 'target' are already defined as per your dataset\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 16)\n",
    "    \n",
    "    # Initialize the RandomForestClassifier with suggested hyperparameters\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                 min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                 random_state=42)\n",
    "    \n",
    "    # Create StratifiedKFold object with 'best_k' folds\n",
    "    skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Use cross-validation to evaluate model performance, passing the SKF object\n",
    "    scores = cross_val_score(clf, features, target, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    # Return the average of the cross-validation scores\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # Adjust n_trials based on your computational budget\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have your final model(s) and you can evaluate them in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.7711323169751445\n",
      "Average Recall: 0.7672747460552909\n",
      "Average Accuracy: 0.7672747460552909\n",
      "Average F1 Score: 0.755905179045945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming features (X) and target (y) are already defined based on your dataset\n",
    "X = merged_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']]\n",
    "y = merged_df['classes']\n",
    "\n",
    "# Optimal parameters from hyperparameter optimization\n",
    "optimal_params = {\n",
    "    'n_estimators': 276,\n",
    "    'max_depth': 32,\n",
    "    'min_samples_split': 12,\n",
    "    'min_samples_leaf': 1,\n",
    "    'random_state': 42  # Ensuring reproducibility\n",
    "}\n",
    "\n",
    "# Initialize the StratifiedKFold object\n",
    "n_splits = best_k  # Assuming best_k is defined based on your previous code\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each fold\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "accuracy_list = []\n",
    "f1_list = []  # If you want to calculate F1 score as well\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Instantiate the RandomForestClassifier with the optimal parameters\n",
    "    classifier = RandomForestClassifier(**optimal_params)\n",
    "\n",
    "    # Train the classifier on the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Calculate and append the evaluation metrics for the current fold\n",
    "    precision_list.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "    recall_list.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "    f1_list.append(f1_score(y_test, y_pred, average='weighted'))  # Calculate F1 score\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "average_precision = np.mean(precision_list)\n",
    "average_recall = np.mean(recall_list)\n",
    "average_accuracy = np.mean(accuracy_list)\n",
    "average_f1 = np.mean(f1_list)  # Average F1 score\n",
    "\n",
    "# Print the average metrics\n",
    "print(f\"Average Precision: {average_precision}\")\n",
    "print(f\"Average Recall: {average_recall}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n",
    "print(f\"Average F1 Score: {average_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You'll also want to output an aggregated prediction confusion matrix (from the cross-validation), preferably as a seaborn (sns) figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming X, y, skf.split, and optimal_params are already defined\n",
    "\n",
    "# Initialize lists to collect true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Your cross-validation and model training process\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    classifier = RandomForestClassifier(**optimal_params)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Collect true and predicted labels\n",
    "    true_labels.extend(y_test)\n",
    "    predicted_labels.extend(y_pred)\n",
    "\n",
    "# Compute the confusion matrix with normalization\n",
    "cm_normalized = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "\n",
    "# Visualize the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\", xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])\n",
    "plt.title('Normalized Aggregated Confusion Matrix Round 1 No. 2')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\klara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"sklearn\\tree\\_tree.pyx\", line 736, in sklearn.tree._tree.Tree.__setstate__\n  File \"sklearn\\tree\\_tree.pyx\", line 771, in sklearn.tree._tree.Tree._resize_c\n  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 5906112 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Perform permutation importance\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get the importance of each feature\u001b[39;00m\n\u001b[0;32m      9\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mimportances_mean\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\inspection\\_permutation_importance.py:289\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[1;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[0;32m    285\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m _MultimetricScorer(scorers\u001b[38;5;241m=\u001b[39mscorers_dict)\n\u001b[0;32m    287\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m _weights_scorer(scorer, estimator, X, y, sample_weight)\n\u001b[1;32m--> 289\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_calculate_permutation_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    306\u001b[0m         name: _create_importances_bunch(\n\u001b[0;32m    307\u001b[0m             baseline_score[name],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m baseline_score\n\u001b[0;32m    312\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "## run feature permutation importance here. \n",
    "## Gives you an indication of which features are the most important for good classification performance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Perform permutation importance\n",
    "result = permutation_importance(classifier, X, y, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get the importance of each feature\n",
    "feature_importance = result.importances_mean\n",
    "\n",
    "# Optionally, sort the features by importance\n",
    "sorted_idx = feature_importance.argsort()\n",
    "\n",
    "# Visualize the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), X.columns[sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa Score: 0.6609883583861486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "kappa_list = []\n",
    "kappa_list.append(cohen_kappa_score(y_test, y_pred))  # Calculate Cohen's Kappa score\n",
    "average_kappa = np.mean(kappa_list)  # Average Cohen's Kappa score\n",
    "print(f\"Average Kappa Score: {average_kappa}\")  # Print Average Cohen's Kappa Score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
