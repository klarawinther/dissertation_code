{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>sample_location_id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05020</td>\n",
       "      <td>0.09065</td>\n",
       "      <td>0.13930</td>\n",
       "      <td>0.15855</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.19205</td>\n",
       "      <td>0.19175</td>\n",
       "      <td>0.20290</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.03980</td>\n",
       "      <td>0.05720</td>\n",
       "      <td>0.10385</td>\n",
       "      <td>0.16755</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.21600</td>\n",
       "      <td>0.23420</td>\n",
       "      <td>0.24700</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01510</td>\n",
       "      <td>0.02905</td>\n",
       "      <td>0.06635</td>\n",
       "      <td>0.04515</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.38280</td>\n",
       "      <td>0.47410</td>\n",
       "      <td>0.49955</td>\n",
       "      <td>0.50775</td>\n",
       "      <td>0.50890</td>\n",
       "      <td>0.24765</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.06315</td>\n",
       "      <td>0.04920</td>\n",
       "      <td>0.12335</td>\n",
       "      <td>0.28515</td>\n",
       "      <td>0.33710</td>\n",
       "      <td>0.39055</td>\n",
       "      <td>0.37655</td>\n",
       "      <td>0.42050</td>\n",
       "      <td>0.23555</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.02970</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.13360</td>\n",
       "      <td>0.39940</td>\n",
       "      <td>0.49485</td>\n",
       "      <td>0.52430</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.48935</td>\n",
       "      <td>0.23950</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152806</th>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.04660</td>\n",
       "      <td>0.06660</td>\n",
       "      <td>0.08560</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.26880</td>\n",
       "      <td>0.28690</td>\n",
       "      <td>0.27960</td>\n",
       "      <td>0.28600</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152807</th>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.07690</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.14000</td>\n",
       "      <td>0.25190</td>\n",
       "      <td>0.28530</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>0.33920</td>\n",
       "      <td>0.32180</td>\n",
       "      <td>0.29830</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152808</th>\n",
       "      <td>0.04280</td>\n",
       "      <td>0.04960</td>\n",
       "      <td>0.06970</td>\n",
       "      <td>0.08880</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.18960</td>\n",
       "      <td>0.21990</td>\n",
       "      <td>0.25540</td>\n",
       "      <td>0.25480</td>\n",
       "      <td>0.25390</td>\n",
       "      <td>0.27920</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152809</th>\n",
       "      <td>0.03170</td>\n",
       "      <td>0.04110</td>\n",
       "      <td>0.06480</td>\n",
       "      <td>0.07660</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>0.23340</td>\n",
       "      <td>0.27290</td>\n",
       "      <td>0.30900</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.29640</td>\n",
       "      <td>0.30290</td>\n",
       "      <td>0.1617</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152810</th>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.04140</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.28990</td>\n",
       "      <td>0.33570</td>\n",
       "      <td>0.38040</td>\n",
       "      <td>0.39880</td>\n",
       "      <td>0.40650</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152811 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              B1       B2       B3       B4       B5       B6       B7  \\\n",
       "0        0.00495  0.01885  0.03610  0.05020  0.09065  0.13930  0.15855   \n",
       "1        0.01040  0.02405  0.03980  0.05720  0.10385  0.16755  0.19370   \n",
       "2        0.01510  0.02905  0.06635  0.04515  0.12920  0.38280  0.47410   \n",
       "3        0.01345  0.02925  0.06315  0.04920  0.12335  0.28515  0.33710   \n",
       "4        0.01575  0.02970  0.06900  0.04870  0.13360  0.39940  0.49485   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "1152806  0.03720  0.04660  0.06660  0.08560  0.13300  0.21200  0.24360   \n",
       "1152807  0.03840  0.04250  0.07690  0.08000  0.14000  0.25190  0.28530   \n",
       "1152808  0.04280  0.04960  0.06970  0.08880  0.12820  0.18960  0.21990   \n",
       "1152809  0.03170  0.04110  0.06480  0.07660  0.13350  0.23340  0.27290   \n",
       "1152810  0.03540  0.04140  0.08000  0.07390  0.15220  0.28990  0.33570   \n",
       "\n",
       "              B8      B8A       B9      B11     B12  sample_location_id  class  \n",
       "0        0.18270  0.19205  0.19175  0.20290  0.1097              201701      2  \n",
       "1        0.21600  0.23420  0.24700  0.22290  0.1204              201701      2  \n",
       "2        0.49955  0.50775  0.50890  0.24765  0.1219              201701      2  \n",
       "3        0.39055  0.37655  0.42050  0.23555  0.1175              201701      2  \n",
       "4        0.52430  0.53900  0.48935  0.23950  0.1177              201701      2  \n",
       "...          ...      ...      ...      ...     ...                 ...    ...  \n",
       "1152806  0.26880  0.28690  0.27960  0.28600  0.1640              202312      2  \n",
       "1152807  0.34760  0.33920  0.32180  0.29830  0.1563              202312      2  \n",
       "1152808  0.25540  0.25480  0.25390  0.27920  0.1567              202312      2  \n",
       "1152809  0.30900  0.30940  0.29640  0.30290  0.1617              202312      2  \n",
       "1152810  0.38040  0.39880  0.40650  0.28320  0.1495              202312      2  \n",
       "\n",
       "[1152811 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simply reading the formerly created pickle file, changing it's name to 'merged_df'\n",
    "# To confirm, you can load and view the combined dataset\n",
    "with open(\"combined_dataset.pkl\", 'rb') as file:\n",
    "    merged_df = pd.read_pickle(file)\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>sample_location_id</th>\n",
       "      <th>class</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05020</td>\n",
       "      <td>0.09065</td>\n",
       "      <td>0.13930</td>\n",
       "      <td>0.15855</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.19205</td>\n",
       "      <td>0.19175</td>\n",
       "      <td>0.20290</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.568914</td>\n",
       "      <td>0.246737</td>\n",
       "      <td>-0.052386</td>\n",
       "      <td>0.271183</td>\n",
       "      <td>0.670018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.03980</td>\n",
       "      <td>0.05720</td>\n",
       "      <td>0.10385</td>\n",
       "      <td>0.16755</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.21600</td>\n",
       "      <td>0.23420</td>\n",
       "      <td>0.24700</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.581259</td>\n",
       "      <td>0.287926</td>\n",
       "      <td>-0.015721</td>\n",
       "      <td>0.308070</td>\n",
       "      <td>0.688819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01510</td>\n",
       "      <td>0.02905</td>\n",
       "      <td>0.06635</td>\n",
       "      <td>0.04515</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.38280</td>\n",
       "      <td>0.47410</td>\n",
       "      <td>0.49955</td>\n",
       "      <td>0.50775</td>\n",
       "      <td>0.50890</td>\n",
       "      <td>0.24765</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.834221</td>\n",
       "      <td>0.731688</td>\n",
       "      <td>0.337125</td>\n",
       "      <td>0.652436</td>\n",
       "      <td>0.765506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.06315</td>\n",
       "      <td>0.04920</td>\n",
       "      <td>0.12335</td>\n",
       "      <td>0.28515</td>\n",
       "      <td>0.33710</td>\n",
       "      <td>0.39055</td>\n",
       "      <td>0.37655</td>\n",
       "      <td>0.42050</td>\n",
       "      <td>0.23555</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.776236</td>\n",
       "      <td>0.581962</td>\n",
       "      <td>0.247564</td>\n",
       "      <td>0.544852</td>\n",
       "      <td>0.721622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.02970</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.13360</td>\n",
       "      <td>0.39940</td>\n",
       "      <td>0.49485</td>\n",
       "      <td>0.52430</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.48935</td>\n",
       "      <td>0.23950</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.830017</td>\n",
       "      <td>0.746039</td>\n",
       "      <td>0.372873</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.767403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152806</th>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.04660</td>\n",
       "      <td>0.06660</td>\n",
       "      <td>0.08560</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.26880</td>\n",
       "      <td>0.28690</td>\n",
       "      <td>0.27960</td>\n",
       "      <td>0.28600</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.516930</td>\n",
       "      <td>0.319632</td>\n",
       "      <td>-0.031002</td>\n",
       "      <td>0.321629</td>\n",
       "      <td>0.602862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152807</th>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.07690</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.14000</td>\n",
       "      <td>0.25190</td>\n",
       "      <td>0.28530</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>0.33920</td>\n",
       "      <td>0.32180</td>\n",
       "      <td>0.29830</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625818</td>\n",
       "      <td>0.443384</td>\n",
       "      <td>0.076328</td>\n",
       "      <td>0.432730</td>\n",
       "      <td>0.637691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152808</th>\n",
       "      <td>0.04280</td>\n",
       "      <td>0.04960</td>\n",
       "      <td>0.06970</td>\n",
       "      <td>0.08880</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.18960</td>\n",
       "      <td>0.21990</td>\n",
       "      <td>0.25540</td>\n",
       "      <td>0.25480</td>\n",
       "      <td>0.25390</td>\n",
       "      <td>0.27920</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484021</td>\n",
       "      <td>0.294097</td>\n",
       "      <td>-0.044519</td>\n",
       "      <td>0.296020</td>\n",
       "      <td>0.571209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152809</th>\n",
       "      <td>0.03170</td>\n",
       "      <td>0.04110</td>\n",
       "      <td>0.06480</td>\n",
       "      <td>0.07660</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>0.23340</td>\n",
       "      <td>0.27290</td>\n",
       "      <td>0.30900</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.29640</td>\n",
       "      <td>0.30290</td>\n",
       "      <td>0.1617</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.602697</td>\n",
       "      <td>0.397850</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.393631</td>\n",
       "      <td>0.653291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152810</th>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.04140</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.28990</td>\n",
       "      <td>0.33570</td>\n",
       "      <td>0.38040</td>\n",
       "      <td>0.39880</td>\n",
       "      <td>0.40650</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.674664</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.146474</td>\n",
       "      <td>0.481767</td>\n",
       "      <td>0.652476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152811 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              B1       B2       B3       B4       B5       B6       B7  \\\n",
       "0        0.00495  0.01885  0.03610  0.05020  0.09065  0.13930  0.15855   \n",
       "1        0.01040  0.02405  0.03980  0.05720  0.10385  0.16755  0.19370   \n",
       "2        0.01510  0.02905  0.06635  0.04515  0.12920  0.38280  0.47410   \n",
       "3        0.01345  0.02925  0.06315  0.04920  0.12335  0.28515  0.33710   \n",
       "4        0.01575  0.02970  0.06900  0.04870  0.13360  0.39940  0.49485   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "1152806  0.03720  0.04660  0.06660  0.08560  0.13300  0.21200  0.24360   \n",
       "1152807  0.03840  0.04250  0.07690  0.08000  0.14000  0.25190  0.28530   \n",
       "1152808  0.04280  0.04960  0.06970  0.08880  0.12820  0.18960  0.21990   \n",
       "1152809  0.03170  0.04110  0.06480  0.07660  0.13350  0.23340  0.27290   \n",
       "1152810  0.03540  0.04140  0.08000  0.07390  0.15220  0.28990  0.33570   \n",
       "\n",
       "              B8      B8A       B9      B11     B12  sample_location_id  \\\n",
       "0        0.18270  0.19205  0.19175  0.20290  0.1097              201701   \n",
       "1        0.21600  0.23420  0.24700  0.22290  0.1204              201701   \n",
       "2        0.49955  0.50775  0.50890  0.24765  0.1219              201701   \n",
       "3        0.39055  0.37655  0.42050  0.23555  0.1175              201701   \n",
       "4        0.52430  0.53900  0.48935  0.23950  0.1177              201701   \n",
       "...          ...      ...      ...      ...     ...                 ...   \n",
       "1152806  0.26880  0.28690  0.27960  0.28600  0.1640              202312   \n",
       "1152807  0.34760  0.33920  0.32180  0.29830  0.1563              202312   \n",
       "1152808  0.25540  0.25480  0.25390  0.27920  0.1567              202312   \n",
       "1152809  0.30900  0.30940  0.29640  0.30290  0.1617              202312   \n",
       "1152810  0.38040  0.39880  0.40650  0.28320  0.1495              202312   \n",
       "\n",
       "         class      NDVI       EVI      NDWI      SAVI     GNDVI  \n",
       "0            2  0.568914  0.246737 -0.052386  0.271183  0.670018  \n",
       "1            2  0.581259  0.287926 -0.015721  0.308070  0.688819  \n",
       "2            2  0.834221  0.731688  0.337125  0.652436  0.765506  \n",
       "3            2  0.776236  0.581962  0.247564  0.544852  0.721622  \n",
       "4            2  0.830017  0.746039  0.372873  0.664865  0.767403  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "1152806      2  0.516930  0.319632 -0.031002  0.321629  0.602862  \n",
       "1152807      2  0.625818  0.443384  0.076328  0.432730  0.637691  \n",
       "1152808      2  0.484021  0.294097 -0.044519  0.296020  0.571209  \n",
       "1152809      2  0.602697  0.397850  0.009969  0.393631  0.653291  \n",
       "1152810      2  0.674664  0.506344  0.146474  0.481767  0.652476  \n",
       "\n",
       "[1152811 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['NDVI'] = (merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + merged_df['B4'])\n",
    "\n",
    "# EVI is an optimized vegetation index designed to enhance the vegetation signal with improved sensitivity in high biomass regions\n",
    "# It's calculated using the Red (B4), Near-Infrared (B8 or B5), and Blue (B2) bands.\n",
    "merged_df['EVI'] = 2.5 * (merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + 6 * merged_df['B4'] - 7.5 * merged_df['B2'] + 1)\n",
    "\n",
    "# NDWI is used to monitor changes in water content of leaves\n",
    "# It is typically calculated using the Near-Infrared (B8 or B5) and Short-Wave Infrared (B11 or B6) bands.\n",
    "merged_df['NDWI'] = (merged_df['B8'] - merged_df['B11']) / (merged_df['B8'] + merged_df['B11'])\n",
    "\n",
    "# SAVI is a modification of NDVI to correct for the influence of soil brightness\n",
    "# The standard value of L in the SAVI formula is 0.5.\n",
    "L = 0.5  # soil brightness correction factor\n",
    "merged_df['SAVI'] = ((merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + merged_df['B4'] + L)) * (1 + L)\n",
    "\n",
    "# GNDVI is used to estimate vegetation health\n",
    "# It's calculated using the Near-Infrared (B8 or B5) and Green (B3) bands.\n",
    "merged_df['GNDVI'] = (merged_df['B8'] - merged_df['B3']) / (merged_df['B8'] + merged_df['B3'])\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming 'class' column to 'classes' to fix python error\n",
    "merged_df = merged_df.rename(columns={'class': 'classes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, I will make some new dataframes for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a new DataFrame for binary classification\n",
    "# First, filter out class 3, then collapse classes 0, 1, and 2 into 0.\n",
    "binary_df = merged_df[merged_df['classes'].isin([0, 1, 2, 4])]\n",
    "binary_df.loc[binary_df['classes'].isin([0, 1, 2]), 'classes'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_df = binary_df.drop('sample_location_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>classes</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03440</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0646</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>0.1222</td>\n",
       "      <td>0.20300</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.2618</td>\n",
       "      <td>0.2672</td>\n",
       "      <td>0.2448</td>\n",
       "      <td>0.24540</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>4</td>\n",
       "      <td>0.569544</td>\n",
       "      <td>0.342379</td>\n",
       "      <td>0.032334</td>\n",
       "      <td>0.341891</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03890</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>0.13680</td>\n",
       "      <td>0.1612</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.1899</td>\n",
       "      <td>0.2191</td>\n",
       "      <td>0.25770</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0</td>\n",
       "      <td>0.489950</td>\n",
       "      <td>0.229853</td>\n",
       "      <td>-0.183196</td>\n",
       "      <td>0.237547</td>\n",
       "      <td>0.603425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03040</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>0.34040</td>\n",
       "      <td>0.4195</td>\n",
       "      <td>0.4156</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.22420</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.816037</td>\n",
       "      <td>0.676532</td>\n",
       "      <td>0.299156</td>\n",
       "      <td>0.584995</td>\n",
       "      <td>0.726631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03560</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.0730</td>\n",
       "      <td>0.1289</td>\n",
       "      <td>0.19880</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.2456</td>\n",
       "      <td>0.2668</td>\n",
       "      <td>0.2672</td>\n",
       "      <td>0.28880</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541745</td>\n",
       "      <td>0.318780</td>\n",
       "      <td>-0.080838</td>\n",
       "      <td>0.316272</td>\n",
       "      <td>0.598958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01920</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.30050</td>\n",
       "      <td>0.3457</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.4086</td>\n",
       "      <td>0.2304</td>\n",
       "      <td>0.22980</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0</td>\n",
       "      <td>0.743328</td>\n",
       "      <td>0.581349</td>\n",
       "      <td>0.240833</td>\n",
       "      <td>0.516113</td>\n",
       "      <td>0.692654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096431</th>\n",
       "      <td>0.04060</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0668</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>0.2702</td>\n",
       "      <td>0.2836</td>\n",
       "      <td>0.2701</td>\n",
       "      <td>0.25570</td>\n",
       "      <td>0.1412</td>\n",
       "      <td>4</td>\n",
       "      <td>0.603561</td>\n",
       "      <td>0.374793</td>\n",
       "      <td>0.027572</td>\n",
       "      <td>0.364516</td>\n",
       "      <td>0.662258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096432</th>\n",
       "      <td>0.02790</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.27580</td>\n",
       "      <td>0.3239</td>\n",
       "      <td>0.3592</td>\n",
       "      <td>0.3802</td>\n",
       "      <td>0.3341</td>\n",
       "      <td>0.21140</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756050</td>\n",
       "      <td>0.547104</td>\n",
       "      <td>0.259026</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.705603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096433</th>\n",
       "      <td>0.03400</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.0677</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>0.17110</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.2266</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.2169</td>\n",
       "      <td>0.22240</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>4</td>\n",
       "      <td>0.539925</td>\n",
       "      <td>0.301449</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.300076</td>\n",
       "      <td>0.595213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096434</th>\n",
       "      <td>0.02255</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.20825</td>\n",
       "      <td>0.2438</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.2634</td>\n",
       "      <td>0.2628</td>\n",
       "      <td>0.17935</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813726</td>\n",
       "      <td>0.405686</td>\n",
       "      <td>0.106266</td>\n",
       "      <td>0.401182</td>\n",
       "      <td>0.691429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096435</th>\n",
       "      <td>0.03190</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.18020</td>\n",
       "      <td>0.2021</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.2359</td>\n",
       "      <td>0.2344</td>\n",
       "      <td>0.22550</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.296614</td>\n",
       "      <td>-0.037975</td>\n",
       "      <td>0.292035</td>\n",
       "      <td>0.572611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096436 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              B1      B2      B3      B4      B5       B6      B7      B8  \\\n",
       "0        0.03440  0.0407  0.0646  0.0718  0.1222  0.20300  0.2303  0.2618   \n",
       "1        0.03890  0.0361  0.0440  0.0609  0.0943  0.13680  0.1612  0.1779   \n",
       "2        0.03040  0.0384  0.0658  0.0421  0.1247  0.34040  0.4195  0.4156   \n",
       "3        0.03560  0.0440  0.0616  0.0730  0.1289  0.19880  0.2284  0.2456   \n",
       "4        0.01920  0.0440  0.0682  0.0553  0.1340  0.30050  0.3457  0.3756   \n",
       "...          ...     ...     ...     ...     ...      ...     ...     ...   \n",
       "1096431  0.04060  0.0419  0.0549  0.0668  0.1226  0.20600  0.2362  0.2702   \n",
       "1096432  0.02790  0.0327  0.0620  0.0499  0.1269  0.27580  0.3239  0.3592   \n",
       "1096433  0.03400  0.0420  0.0575  0.0677  0.1048  0.17110  0.1987  0.2266   \n",
       "1096434  0.02255  0.0175  0.0405  0.0228  0.0991  0.20825  0.2438  0.2220   \n",
       "1096435  0.03190  0.0406  0.0568  0.0594  0.1051  0.18020  0.2021  0.2090   \n",
       "\n",
       "            B8A      B9      B11     B12  classes      NDVI       EVI  \\\n",
       "0        0.2672  0.2448  0.24540  0.1302        4  0.569544  0.342379   \n",
       "1        0.1899  0.2191  0.25770  0.1609        0  0.489950  0.229853   \n",
       "2        0.4835  0.4696  0.22420  0.1009        0  0.816037  0.676532   \n",
       "3        0.2668  0.2672  0.28880  0.1660        0  0.541745  0.318780   \n",
       "4        0.4086  0.2304  0.22980  0.1123        0  0.743328  0.581349   \n",
       "...         ...     ...      ...     ...      ...       ...       ...   \n",
       "1096431  0.2836  0.2701  0.25570  0.1412        4  0.603561  0.374793   \n",
       "1096432  0.3802  0.3341  0.21140  0.1023        0  0.756050  0.547104   \n",
       "1096433  0.2301  0.2169  0.22240  0.1283        4  0.539925  0.301449   \n",
       "1096434  0.2634  0.2628  0.17935  0.0919        0  0.813726  0.405686   \n",
       "1096435  0.2359  0.2344  0.22550  0.1302        0  0.557377  0.296614   \n",
       "\n",
       "             NDWI      SAVI     GNDVI  \n",
       "0        0.032334  0.341891  0.604167  \n",
       "1       -0.183196  0.237547  0.603425  \n",
       "2        0.299156  0.584995  0.726631  \n",
       "3       -0.080838  0.316272  0.598958  \n",
       "4        0.240833  0.516113  0.692654  \n",
       "...           ...       ...       ...  \n",
       "1096431  0.027572  0.364516  0.662258  \n",
       "1096432  0.259026  0.510340  0.705603  \n",
       "1096433  0.009354  0.300076  0.595213  \n",
       "1096434  0.106266  0.401182  0.691429  \n",
       "1096435 -0.037975  0.292035  0.572611  \n",
       "\n",
       "[1096436 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming merged_df is your pandas DataFrame\n",
    "# Shuffle the DataFrame using a random seed, for example, seed=42\n",
    "binary_df = binary_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "binary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your features are all columns except 'classes', and 'classes' is the target variable\n",
    "X = binary_df.drop('classes', axis=1)  # Features\n",
    "y = binary_df['classes']  # Target variable\n",
    "\n",
    "# Perform the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# X_train and y_train will now contain 70% of the data, X_test and y_test will contain 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of binary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this bit I will do a classification based on binary_df which contains classes 0 and 4.\n",
    "Class 0 is a collapsed class based on the peatland conditions 'actively eroding', 'drained', and 'modified', while class 4 is still 'restored'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 400)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1.0)\n",
    "    min_samples_leaf = trial.suggest_float('min_samples_leaf', 0.1, 0.5)\n",
    "    \n",
    "    \n",
    "    # Initialize the classifier with the current hyperparameters\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    # Compute and return the accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-12 12:15:34,124] A new study created in memory with name: no-name-ea91c067-eb47-4124-a76e-3008f1f7477b\n",
      "[I 2024-02-12 12:16:22,291] Trial 0 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 116, 'max_depth': 6, 'min_samples_split': 0.32282974849030754, 'min_samples_leaf': 0.1915728313284759}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:16:38,609] Trial 1 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 209, 'max_depth': 4, 'min_samples_split': 0.9066794928636984, 'min_samples_leaf': 0.2176713609821563}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:16:53,721] Trial 2 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 183, 'max_depth': 28, 'min_samples_split': 0.18151969269520807, 'min_samples_leaf': 0.4678307131532643}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:17:04,646] Trial 3 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 136, 'max_depth': 27, 'min_samples_split': 0.16373459826705916, 'min_samples_leaf': 0.4309154102093794}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:18:34,331] Trial 4 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 278, 'max_depth': 13, 'min_samples_split': 0.5564262527742393, 'min_samples_leaf': 0.11813066554296224}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:18:52,369] Trial 5 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 230, 'max_depth': 5, 'min_samples_split': 0.6773235387184874, 'min_samples_leaf': 0.45780863910855674}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:19:09,759] Trial 6 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 217, 'max_depth': 4, 'min_samples_split': 0.46413634421563743, 'min_samples_leaf': 0.44928892298236545}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:19:39,867] Trial 7 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 372, 'max_depth': 7, 'min_samples_split': 0.899560879006323, 'min_samples_leaf': 0.29743239497730634}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:19:57,018] Trial 8 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 214, 'max_depth': 32, 'min_samples_split': 0.306523392915405, 'min_samples_leaf': 0.4892175417930188}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:20:22,566] Trial 9 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 292, 'max_depth': 2, 'min_samples_split': 0.9089258302822748, 'min_samples_leaf': 0.32609027469891805}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:21:12,034] Trial 10 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 81, 'max_depth': 11, 'min_samples_split': 0.3652696662617758, 'min_samples_leaf': 0.11343238430586108}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:21:17,144] Trial 11 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 51, 'max_depth': 2, 'min_samples_split': 0.7235236377561204, 'min_samples_leaf': 0.22271958077371146}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:21:29,835] Trial 12 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 136, 'max_depth': 3, 'min_samples_split': 0.7386586495117942, 'min_samples_leaf': 0.19437853701036145}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:21:41,440] Trial 13 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 133, 'max_depth': 8, 'min_samples_split': 0.983772655910194, 'min_samples_leaf': 0.21502999284120944}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:23:19,119] Trial 14 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 322, 'max_depth': 4, 'min_samples_split': 0.557626246606059, 'min_samples_leaf': 0.28348514064198316}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:24:43,393] Trial 15 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 170, 'max_depth': 14, 'min_samples_split': 0.317123014390126, 'min_samples_leaf': 0.16929105479478398}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:24:50,904] Trial 16 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 85, 'max_depth': 3, 'min_samples_split': 0.4241621752140411, 'min_samples_leaf': 0.37263735276989585}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:26:11,455] Trial 17 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 262, 'max_depth': 5, 'min_samples_split': 0.1096396392749667, 'min_samples_leaf': 0.24454682770058242}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:27:05,610] Trial 18 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 169, 'max_depth': 9, 'min_samples_split': 0.6249279557596896, 'min_samples_leaf': 0.16129926779631615}. Best is trial 0 with value: 0.7720038549118204.\n",
      "[I 2024-02-12 12:27:14,536] Trial 19 finished with value: 0.7720038549118204 and parameters: {'n_estimators': 106, 'max_depth': 19, 'min_samples_split': 0.8138759724689874, 'min_samples_leaf': 0.2687775429351759}. Best is trial 0 with value: 0.7720038549118204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'n_estimators': 116, 'max_depth': 6, 'min_samples_split': 0.32282974849030754, 'min_samples_leaf': 0.1915728313284759}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF model based on bands only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "features = binary_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']].values\n",
    "target = binary_df['classes'].values\n",
    "\n",
    "# Assuming features and target are already defined\n",
    "skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)\n",
    "\n",
    "rf_bands_binary_1_accuracy_scores = []\n",
    "rf_bands_binary_1_precision_scores = []\n",
    "rf_bands_binary_1_recall_scores = []\n",
    "rf_bands_binary_1_f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(features, target):\n",
    "    rf_bands_binary_1_X_train, rf_bands_binary_1_X_test = features[train_index], features[test_index]\n",
    "    rf_bands_binary_1_y_train, rf_bands_binary_1_y_test = target[train_index], target[test_index]\n",
    "\n",
    "    rf_bands_binary_1_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_bands_binary_1_model.fit(rf_bands_binary_1_X_train, rf_bands_binary_1_y_train)\n",
    "\n",
    "    rf_bands_binary_1_y_pred = rf_bands_binary_1_model.predict(rf_bands_binary_1_X_test)\n",
    "\n",
    "    rf_bands_binary_1_accuracy_scores.append(accuracy_score(rf_bands_binary_1_y_test, rf_bands_binary_1_y_pred))\n",
    "    rf_bands_binary_1_precision_scores.append(precision_score(rf_bands_binary_1_y_test, rf_bands_binary_1_y_pred, average='macro'))\n",
    "    rf_bands_binary_1_recall_scores.append(recall_score(rf_bands_binary_1_y_test, rf_bands_binary_1_y_pred, average='macro'))\n",
    "    rf_bands_binary_1_f1_scores.append(f1_score(rf_bands_binary_1_y_test, rf_bands_binary_1_y_pred, average='macro'))\n",
    "\n",
    "# Calculate the average scores across all folds\n",
    "rf_bands_binary_1_average_accuracy = np.mean(rf_bands_binary_1_accuracy_scores)\n",
    "rf_bands_binary_1_average_precision = np.mean(rf_bands_binary_1_precision_scores)\n",
    "rf_bands_binary_1_average_recall = np.mean(rf_bands_binary_1_recall_scores)\n",
    "rf_bands_binary_1_average_f1 = np.mean(rf_bands_binary_1_f1_scores)\n",
    "\n",
    "print(f\"Binary 1 RF Bands Average Accuracy: {rf_bands_binary_1_average_accuracy}\")\n",
    "print(f\"Binary 1 RF Bands Average Precision: {rf_bands_binary_1_average_precision}\")\n",
    "print(f\"Binary 1 RF Bands Average Recall: {rf_bands_binary_1_average_recall}\")\n",
    "print(f\"Binary 1 RF Bands Average F1 Score: {rf_bands_binary_1_average_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF model based on both bands and VIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = binary_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']].values\n",
    "target = binary_df['classes'].values\n",
    "\n",
    "# Assuming features and target are already defined\n",
    "skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)\n",
    "\n",
    "rf_all_binary_1_accuracy_scores = []\n",
    "rf_all_binary_1_precision_scores = []\n",
    "rf_all_binary_1_recall_scores = []\n",
    "rf_all_binary_1_f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(features, target):\n",
    "    rf_all_binary_1_X_train, rf_all_binary_1_X_test = features[train_index], features[test_index]\n",
    "    rf_all_binary_1_y_train, rf_all_binary_1_y_test = target[train_index], target[test_index]\n",
    "\n",
    "    rf_all_binary_1_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_all_binary_1_model.fit(rf_all_binary_1_X_train, rf_all_binary_1_y_train)\n",
    "\n",
    "    rf_all_binary_1_y_pred = rf_all_binary_1_model.predict(rf_all_binary_1_X_test)\n",
    "\n",
    "    rf_all_binary_1_accuracy_scores.append(accuracy_score(rf_all_binary_1_y_test, rf_all_binary_1_y_pred))\n",
    "    rf_all_binary_1_precision_scores.append(precision_score(rf_all_binary_1_y_test, rf_all_binary_1_y_pred, average='macro'))\n",
    "    rf_all_binary_1_recall_scores.append(recall_score(rf_all_binary_1_y_test, rf_all_binary_1_y_pred, average='macro'))\n",
    "    rf_all_binary_1_f1_scores.append(f1_score(rf_all_binary_1_y_test, rf_all_binary_1_y_pred, average='macro'))\n",
    "\n",
    "# Calculate the average scores across all folds\n",
    "rf_all_binary_1_average_accuracy = np.mean(rf_all_binary_1_accuracy_scores)\n",
    "rf_all_binary_1_average_precision = np.mean(rf_all_binary_1_precision_scores)\n",
    "rf_all_binary_1_average_recall = np.mean(rf_all_binary_1_recall_scores)\n",
    "rf_all_binary_1_average_f1 = np.mean(rf_all_binary_1_f1_scores)\n",
    "\n",
    "print(f\"Binary 1 RF Bands and VIs Average Accuracy: {rf_all_binary_1_average_accuracy}\")\n",
    "print(f\"Binary 1 RF Bands and VIs Average Precision: {rf_all_binary_1_average_precision}\")\n",
    "print(f\"Binary 1 RF Bands and VIs Average Recall: {rf_all_binary_1_average_recall}\")\n",
    "print(f\"Binary 1 RF Bands and VIs Average F1 Score: {rf_all_binary_1_average_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF Optuna hyperparameter optimisation based on only bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-02-08 16:52:18,854] A new study created in memory with name: no-name-b133f83a-24ee-472e-acba-d2abe3bce4bc\n",
      "[I 2024-02-08 17:44:04,956] Trial 0 finished with value: 0.86581432933614 and parameters: {'n_estimators': 337, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 13}. Best is trial 0 with value: 0.86581432933614.\n",
      "[I 2024-02-08 18:22:28,791] Trial 1 finished with value: 0.8423711005475923 and parameters: {'n_estimators': 477, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 13}. Best is trial 0 with value: 0.86581432933614.\n",
      "[I 2024-02-08 18:30:33,231] Trial 2 finished with value: 0.7864508279553024 and parameters: {'n_estimators': 465, 'max_depth': 2, 'min_samples_split': 8, 'min_samples_leaf': 16}. Best is trial 0 with value: 0.86581432933614.\n",
      "[I 2024-02-08 19:37:28,865] Trial 3 finished with value: 0.9246896307673225 and parameters: {'n_estimators': 449, 'max_depth': 25, 'min_samples_split': 16, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9246896307673225.\n",
      "[I 2024-02-08 19:44:14,639] Trial 4 finished with value: 0.9017006008558639 and parameters: {'n_estimators': 90, 'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9246896307673225.\n",
      "[I 2024-02-08 19:45:22,498] Trial 5 finished with value: 0.7928169086020525 and parameters: {'n_estimators': 37, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 14}. Best is trial 3 with value: 0.9246896307673225.\n",
      "[I 2024-02-08 20:41:45,419] Trial 6 finished with value: 0.8328821746093708 and parameters: {'n_estimators': 376, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9246896307673225.\n",
      "[I 2024-02-08 22:26:22,008] Trial 7 finished with value: 0.9256253898996385 and parameters: {'n_estimators': 252, 'max_depth': 24, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 7 with value: 0.9256253898996385.\n",
      "[I 2024-02-08 22:46:05,205] Trial 8 finished with value: 0.8085196035153899 and parameters: {'n_estimators': 179, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 7}. Best is trial 7 with value: 0.9256253898996385.\n",
      "[I 2024-02-08 22:53:36,301] Trial 9 finished with value: 0.8149404069184156 and parameters: {'n_estimators': 161, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 7 with value: 0.9256253898996385.\n",
      "[I 2024-02-08 23:51:36,860] Trial 10 finished with value: 0.9559782787139423 and parameters: {'n_estimators': 278, 'max_depth': 32, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 00:37:37,399] Trial 11 finished with value: 0.9550297509384953 and parameters: {'n_estimators': 274, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 01:14:46,301] Trial 12 finished with value: 0.9540766629333586 and parameters: {'n_estimators': 275, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 01:19:30,425] Trial 13 finished with value: 0.7926746294357354 and parameters: {'n_estimators': 261, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 01:45:01,455] Trial 14 finished with value: 0.904610939443798 and parameters: {'n_estimators': 349, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 02:04:57,898] Trial 15 finished with value: 0.9359716390195141 and parameters: {'n_estimators': 198, 'max_depth': 29, 'min_samples_split': 13, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 02:35:23,664] Trial 16 finished with value: 0.9117221616218366 and parameters: {'n_estimators': 398, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 02:40:56,755] Trial 17 finished with value: 0.7926609487466665 and parameters: {'n_estimators': 309, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 03:00:40,996] Trial 18 finished with value: 0.9245637684278882 and parameters: {'n_estimators': 215, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 10}. Best is trial 10 with value: 0.9559782787139423.\n",
      "[I 2024-02-09 03:09:58,085] Trial 19 finished with value: 0.927775082175339 and parameters: {'n_estimators': 106, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9559782787139423.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'n_estimators': 278, 'max_depth': 32, 'min_samples_split': 2, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = binary_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']].values\n",
    "target = binary_df['classes'].values\n",
    "\n",
    "# Assuming 'best_k' is defined based on your previous analysis\n",
    "# Assuming 'features' and 'target' are already defined as per your dataset\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 16)\n",
    "    \n",
    "    # Initialize the RandomForestClassifier with suggested hyperparameters\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                 min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                 random_state=42)\n",
    "    \n",
    "    # Create StratifiedKFold object with 'best_k' folds\n",
    "    skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Use cross-validation to evaluate model performance, passing the SKF object\n",
    "    scores = cross_val_score(clf, features, target, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    # Return the average of the cross-validation scores\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # Adjust n_trials based on your computational budget\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimisation based on bands and VIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-09 03:09:58,537] A new study created in memory with name: no-name-4578396c-8942-4eb9-a263-d9c84c3d9d9c\n",
      "[I 2024-02-09 03:55:33,325] Trial 0 finished with value: 0.8303667519125604 and parameters: {'n_estimators': 472, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 13}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 03:58:53,585] Trial 1 finished with value: 0.7930376237190315 and parameters: {'n_estimators': 81, 'max_depth': 4, 'min_samples_split': 14, 'min_samples_leaf': 14}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 04:06:05,905] Trial 2 finished with value: 0.7917388703034194 and parameters: {'n_estimators': 332, 'max_depth': 2, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 04:10:20,491] Trial 3 finished with value: 0.7917516389465504 and parameters: {'n_estimators': 199, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 04:19:52,253] Trial 4 finished with value: 0.7917343100737297 and parameters: {'n_estimators': 440, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 15}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 04:32:19,769] Trial 5 finished with value: 0.7922596485339775 and parameters: {'n_estimators': 401, 'max_depth': 3, 'min_samples_split': 12, 'min_samples_leaf': 11}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 04:55:29,493] Trial 6 finished with value: 0.8227931224439913 and parameters: {'n_estimators': 264, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 05:26:45,153] Trial 7 finished with value: 0.8140584584964375 and parameters: {'n_estimators': 399, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 05:48:56,520] Trial 8 finished with value: 0.8228660861190257 and parameters: {'n_estimators': 254, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8303667519125604.\n",
      "[I 2024-02-09 06:04:10,160] Trial 9 finished with value: 0.912574012527863 and parameters: {'n_estimators': 89, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 9 with value: 0.912574012527863.\n",
      "[I 2024-02-09 06:08:57,501] Trial 10 finished with value: 0.9177371045824836 and parameters: {'n_estimators': 26, 'max_depth': 29, 'min_samples_split': 16, 'min_samples_leaf': 8}. Best is trial 10 with value: 0.9177371045824836.\n",
      "[I 2024-02-09 06:13:35,886] Trial 11 finished with value: 0.9177872671090697 and parameters: {'n_estimators': 25, 'max_depth': 30, 'min_samples_split': 16, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 06:15:36,746] Trial 12 finished with value: 0.9115242476533059 and parameters: {'n_estimators': 11, 'max_depth': 30, 'min_samples_split': 16, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 06:33:02,731] Trial 13 finished with value: 0.894943252501742 and parameters: {'n_estimators': 120, 'max_depth': 17, 'min_samples_split': 16, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 06:37:34,214] Trial 14 finished with value: 0.9134121827448205 and parameters: {'n_estimators': 25, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 11}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 06:58:02,436] Trial 15 finished with value: 0.8793299380903217 and parameters: {'n_estimators': 166, 'max_depth': 15, 'min_samples_split': 14, 'min_samples_leaf': 9}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 07:00:49,530] Trial 16 finished with value: 0.7987643601632928 and parameters: {'n_estimators': 60, 'max_depth': 5, 'min_samples_split': 11, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 07:24:19,697] Trial 17 finished with value: 0.910988876687741 and parameters: {'n_estimators': 149, 'max_depth': 23, 'min_samples_split': 16, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 07:49:35,794] Trial 18 finished with value: 0.8633946714628122 and parameters: {'n_estimators': 237, 'max_depth': 13, 'min_samples_split': 14, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.9177872671090697.\n",
      "[I 2024-02-09 07:58:02,891] Trial 19 finished with value: 0.91505568952497 and parameters: {'n_estimators': 52, 'max_depth': 23, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.9177872671090697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'n_estimators': 25, 'max_depth': 30, 'min_samples_split': 16, 'min_samples_leaf': 8}\n"
     ]
    }
   ],
   "source": [
    "features = binary_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']].values\n",
    "target = binary_df['classes'].values\n",
    "\n",
    "# Assuming 'best_k' is defined based on your previous analysis\n",
    "# Assuming 'features' and 'target' are already defined as per your dataset\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 16)\n",
    "    \n",
    "    # Initialize the RandomForestClassifier with suggested hyperparameters\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                 min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                 random_state=42)\n",
    "    \n",
    "    # Create StratifiedKFold object with 'best_k' folds\n",
    "    skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Use cross-validation to evaluate model performance, passing the SKF object\n",
    "    scores = cross_val_score(clf, features, target, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    # Return the average of the cross-validation scores\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # Adjust n_trials based on your computational budget\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for bands only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.9566647167693674\n",
      "Average Recall: 0.9559782787139423\n",
      "Average Accuracy: 0.9559782787139423\n",
      "Average F1 Score: 0.9547020119912459\n",
      "Average Kappa Score: 0.8679483763332183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming features (X) and target (y) are already defined based on your dataset\n",
    "X = binary_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']]\n",
    "y = binary_df['classes']\n",
    "\n",
    "# Best trial: {'n_estimators': 278, 'max_depth': 32, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
    "\n",
    "# Optimal parameters from hyperparameter optimization\n",
    "optimal_params = {\n",
    "    'n_estimators': 278,\n",
    "    'max_depth': 32,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'random_state': 42  # Ensuring reproducibility\n",
    "}\n",
    "\n",
    "# Initialize the StratifiedKFold object\n",
    "n_splits = best_k  # Assuming best_k is defined based on your previous code\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each fold\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "accuracy_list = []\n",
    "f1_list = []  # If you want to calculate F1 score as well\n",
    "kappa_list = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Instantiate the RandomForestClassifier with the optimal parameters\n",
    "    classifier = RandomForestClassifier(**optimal_params)\n",
    "\n",
    "    # Train the classifier on the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Calculate and append the evaluation metrics for the current fold\n",
    "    precision_list.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "    recall_list.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "    f1_list.append(f1_score(y_test, y_pred, average='weighted'))  # Calculate F1 score\n",
    "    kappa_list.append(cohen_kappa_score(y_test, y_pred))  # Calculate Cohen's Kappa score\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "average_precision = np.mean(precision_list)\n",
    "average_recall = np.mean(recall_list)\n",
    "average_accuracy = np.mean(accuracy_list)\n",
    "average_f1 = np.mean(f1_list)  # Average F1 score\n",
    "average_kappa = np.mean(kappa_list)  # Average Cohen's Kappa score\n",
    "\n",
    "# Print the average metrics\n",
    "print(f\"Average Precision: {average_precision}\")\n",
    "print(f\"Average Recall: {average_recall}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n",
    "print(f\"Average F1 Score: {average_f1}\")\n",
    "print(f\"Average Kappa Score: {average_kappa}\")  # Print Average Cohen's Kappa Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You'll also want to output an aggregated prediction confusion matrix (from the cross-validation), preferably as a seaborn (sns) figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the StratifiedKFold object\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to collect true and predicted labels across all folds\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Instantiate and train the RandomForestClassifier with the optimal parameters\n",
    "    classifier = RandomForestClassifier(**optimal_params)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Append true and predicted labels for later aggregation\n",
    "    true_labels.extend(y_test)\n",
    "    predicted_labels.extend(y_pred)\n",
    "\n",
    "# Compute the confusion matrix with normalization\n",
    "cm_normalized = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "\n",
    "# Visualize the aggregated confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Purples\", xticklabels=[0, 4], yticklabels=[0, 4])\n",
    "plt.title('Aggregated Confusion Matrix Round 2 No. 1')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\klara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"sklearn\\tree\\_tree.pyx\", line 736, in sklearn.tree._tree.Tree.__setstate__\n  File \"sklearn\\tree\\_tree.pyx\", line 771, in sklearn.tree._tree.Tree._resize_c\n  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 4394048 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming 'classifier' is your already trained RandomForestClassifier instance\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Perform permutation importance using the pre-trained classifier\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Get the importance of each feature\u001b[39;00m\n\u001b[0;32m     12\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mimportances_mean\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\inspection\\_permutation_importance.py:289\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[1;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[0;32m    285\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m _MultimetricScorer(scorers\u001b[38;5;241m=\u001b[39mscorers_dict)\n\u001b[0;32m    287\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m _weights_scorer(scorer, estimator, X, y, sample_weight)\n\u001b[1;32m--> 289\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_calculate_permutation_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    306\u001b[0m         name: _create_importances_bunch(\n\u001b[0;32m    307\u001b[0m             baseline_score[name],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m baseline_score\n\u001b[0;32m    312\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "## run feature permutation importance here. \n",
    "## Gives you an indication of which features are the most important for good classification performance\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Assuming 'classifier' is your already trained RandomForestClassifier instance\n",
    "\n",
    "# Perform permutation importance using the pre-trained classifier\n",
    "result = permutation_importance(classifier, X, y, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get the importance of each feature\n",
    "feature_importance = result.importances_mean\n",
    "\n",
    "# Optionally, sort the features by importance\n",
    "sorted_idx = feature_importance.argsort()\n",
    "\n",
    "# Visualize the feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center', color='darkpurple')\n",
    "plt.yticks(range(len(sorted_idx)), X.columns[sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance Round 2 No. 1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for bands and VIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.9185526395454972\n",
      "Average Recall: 0.9177872671090697\n",
      "Average Accuracy: 0.9177872671090697\n",
      "Average F1 Score: 0.9132658103739658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming features (X) and target (y) are already defined based on your dataset\n",
    "X = binary_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']]\n",
    "y = binary_df['classes']\n",
    "\n",
    "# Best trial: {'n_estimators': 25, 'max_depth': 30, 'min_samples_split': 16, 'min_samples_leaf': 8}\n",
    "\n",
    "# Optimal parameters from hyperparameter optimization\n",
    "optimal_params = {\n",
    "    'n_estimators': 25,\n",
    "    'max_depth': 30,\n",
    "    'min_samples_split': 16,\n",
    "    'min_samples_leaf': 8,\n",
    "    'random_state': 42  # Ensuring reproducibility\n",
    "}\n",
    "\n",
    "# Initialize the StratifiedKFold object\n",
    "n_splits = best_k  # Assuming best_k is defined based on your previous code\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each fold\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "accuracy_list = []\n",
    "f1_list = []  # If you want to calculate F1 score as well\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Instantiate the RandomForestClassifier with the optimal parameters\n",
    "    classifier = RandomForestClassifier(**optimal_params)\n",
    "\n",
    "    # Train the classifier on the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Calculate and append the evaluation metrics for the current fold\n",
    "    precision_list.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "    recall_list.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "    f1_list.append(f1_score(y_test, y_pred, average='weighted'))  # Calculate F1 score\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "average_precision = np.mean(precision_list)\n",
    "average_recall = np.mean(recall_list)\n",
    "average_accuracy = np.mean(accuracy_list)\n",
    "average_f1 = np.mean(f1_list)  # Average F1 score\n",
    "\n",
    "# Print the average metrics\n",
    "print(f\"Average Precision: {average_precision}\")\n",
    "print(f\"Average Recall: {average_recall}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n",
    "print(f\"Average F1 Score: {average_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming features (X) and target (y) are already defined based on your dataset\n",
    "X = binary_df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']]\n",
    "y = binary_df['classes']\n",
    "\n",
    "# Best trial: {'n_estimators': 25, 'max_depth': 30, 'min_samples_split': 16, 'min_samples_leaf': 8}\n",
    "\n",
    "# Optimal parameters from hyperparameter optimization\n",
    "optimal_params = {\n",
    "    'n_estimators': 25,\n",
    "    'max_depth': 30,\n",
    "    'min_samples_split': 16,\n",
    "    'min_samples_leaf': 8,\n",
    "    'random_state': 42  # Ensuring reproducibility\n",
    "}\n",
    "\n",
    "# Initialize the StratifiedKFold object\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to collect true and predicted labels across all folds\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Instantiate and train the RandomForestClassifier with the optimal parameters\n",
    "    classifier = RandomForestClassifier(**optimal_params)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Append true and predicted labels for later aggregation\n",
    "    true_labels.extend(y_test)\n",
    "    predicted_labels.extend(y_pred)\n",
    "\n",
    "# Compute the confusion matrix with normalization\n",
    "cm_normalized = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "\n",
    "# Visualize the aggregated confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\", xticklabels=[0, 4], yticklabels=[0, 4])\n",
    "plt.title('Aggregated Confusion Matrix Round 2 No. 2')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa Score: 0.7446579066679935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "kappa_list = []\n",
    "kappa_list.append(cohen_kappa_score(y_test, y_pred))  # Calculate Cohen's Kappa score\n",
    "average_kappa = np.mean(kappa_list)  # Average Cohen's Kappa score\n",
    "print(f\"Average Kappa Score: {average_kappa}\")  # Print Average Cohen's Kappa Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
