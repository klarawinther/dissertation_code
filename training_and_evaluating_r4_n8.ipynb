{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation notebook\n",
    "Round 4, no. 7 binary classification based on class 1 (drained) and 4 (restored) from the original dataset and trained on bands and VIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>sample_location_id</th>\n",
       "      <th>classes</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05020</td>\n",
       "      <td>0.09065</td>\n",
       "      <td>0.13930</td>\n",
       "      <td>0.15855</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.19205</td>\n",
       "      <td>0.19175</td>\n",
       "      <td>0.20290</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.568914</td>\n",
       "      <td>0.246737</td>\n",
       "      <td>-0.052386</td>\n",
       "      <td>0.271183</td>\n",
       "      <td>0.670018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.03980</td>\n",
       "      <td>0.05720</td>\n",
       "      <td>0.10385</td>\n",
       "      <td>0.16755</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.21600</td>\n",
       "      <td>0.23420</td>\n",
       "      <td>0.24700</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.581259</td>\n",
       "      <td>0.287926</td>\n",
       "      <td>-0.015721</td>\n",
       "      <td>0.308070</td>\n",
       "      <td>0.688819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01510</td>\n",
       "      <td>0.02905</td>\n",
       "      <td>0.06635</td>\n",
       "      <td>0.04515</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.38280</td>\n",
       "      <td>0.47410</td>\n",
       "      <td>0.49955</td>\n",
       "      <td>0.50775</td>\n",
       "      <td>0.50890</td>\n",
       "      <td>0.24765</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.834221</td>\n",
       "      <td>0.731688</td>\n",
       "      <td>0.337125</td>\n",
       "      <td>0.652436</td>\n",
       "      <td>0.765506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.06315</td>\n",
       "      <td>0.04920</td>\n",
       "      <td>0.12335</td>\n",
       "      <td>0.28515</td>\n",
       "      <td>0.33710</td>\n",
       "      <td>0.39055</td>\n",
       "      <td>0.37655</td>\n",
       "      <td>0.42050</td>\n",
       "      <td>0.23555</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.776236</td>\n",
       "      <td>0.581962</td>\n",
       "      <td>0.247564</td>\n",
       "      <td>0.544852</td>\n",
       "      <td>0.721622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.02970</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.13360</td>\n",
       "      <td>0.39940</td>\n",
       "      <td>0.49485</td>\n",
       "      <td>0.52430</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.48935</td>\n",
       "      <td>0.23950</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.830017</td>\n",
       "      <td>0.746039</td>\n",
       "      <td>0.372872</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.767403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152805</th>\n",
       "      <td>1152806</td>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.04660</td>\n",
       "      <td>0.06660</td>\n",
       "      <td>0.08560</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.26880</td>\n",
       "      <td>0.28690</td>\n",
       "      <td>0.27960</td>\n",
       "      <td>0.28600</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.516930</td>\n",
       "      <td>0.319632</td>\n",
       "      <td>-0.031002</td>\n",
       "      <td>0.321629</td>\n",
       "      <td>0.602862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152806</th>\n",
       "      <td>1152807</td>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.07690</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.14000</td>\n",
       "      <td>0.25190</td>\n",
       "      <td>0.28530</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>0.33920</td>\n",
       "      <td>0.32180</td>\n",
       "      <td>0.29830</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625819</td>\n",
       "      <td>0.443384</td>\n",
       "      <td>0.076328</td>\n",
       "      <td>0.432730</td>\n",
       "      <td>0.637691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152807</th>\n",
       "      <td>1152808</td>\n",
       "      <td>0.04280</td>\n",
       "      <td>0.04960</td>\n",
       "      <td>0.06970</td>\n",
       "      <td>0.08880</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.18960</td>\n",
       "      <td>0.21990</td>\n",
       "      <td>0.25540</td>\n",
       "      <td>0.25480</td>\n",
       "      <td>0.25390</td>\n",
       "      <td>0.27920</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484021</td>\n",
       "      <td>0.294097</td>\n",
       "      <td>-0.044519</td>\n",
       "      <td>0.296020</td>\n",
       "      <td>0.571209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152808</th>\n",
       "      <td>1152809</td>\n",
       "      <td>0.03170</td>\n",
       "      <td>0.04110</td>\n",
       "      <td>0.06480</td>\n",
       "      <td>0.07660</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>0.23340</td>\n",
       "      <td>0.27290</td>\n",
       "      <td>0.30900</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.29640</td>\n",
       "      <td>0.30290</td>\n",
       "      <td>0.1617</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.602697</td>\n",
       "      <td>0.397850</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.393631</td>\n",
       "      <td>0.653291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152809</th>\n",
       "      <td>1152810</td>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.04140</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.28990</td>\n",
       "      <td>0.33570</td>\n",
       "      <td>0.38040</td>\n",
       "      <td>0.39880</td>\n",
       "      <td>0.40650</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.674664</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.146474</td>\n",
       "      <td>0.481767</td>\n",
       "      <td>0.652476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152810 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0       B1       B2       B3       B4       B5       B6  \\\n",
       "0                 0  0.00495  0.01885  0.03610  0.05020  0.09065  0.13930   \n",
       "1                 1  0.01040  0.02405  0.03980  0.05720  0.10385  0.16755   \n",
       "2                 2  0.01510  0.02905  0.06635  0.04515  0.12920  0.38280   \n",
       "3                 3  0.01345  0.02925  0.06315  0.04920  0.12335  0.28515   \n",
       "4                 4  0.01575  0.02970  0.06900  0.04870  0.13360  0.39940   \n",
       "...             ...      ...      ...      ...      ...      ...      ...   \n",
       "1152805     1152806  0.03720  0.04660  0.06660  0.08560  0.13300  0.21200   \n",
       "1152806     1152807  0.03840  0.04250  0.07690  0.08000  0.14000  0.25190   \n",
       "1152807     1152808  0.04280  0.04960  0.06970  0.08880  0.12820  0.18960   \n",
       "1152808     1152809  0.03170  0.04110  0.06480  0.07660  0.13350  0.23340   \n",
       "1152809     1152810  0.03540  0.04140  0.08000  0.07390  0.15220  0.28990   \n",
       "\n",
       "              B7       B8      B8A       B9      B11     B12  \\\n",
       "0        0.15855  0.18270  0.19205  0.19175  0.20290  0.1097   \n",
       "1        0.19370  0.21600  0.23420  0.24700  0.22290  0.1204   \n",
       "2        0.47410  0.49955  0.50775  0.50890  0.24765  0.1219   \n",
       "3        0.33710  0.39055  0.37655  0.42050  0.23555  0.1175   \n",
       "4        0.49485  0.52430  0.53900  0.48935  0.23950  0.1177   \n",
       "...          ...      ...      ...      ...      ...     ...   \n",
       "1152805  0.24360  0.26880  0.28690  0.27960  0.28600  0.1640   \n",
       "1152806  0.28530  0.34760  0.33920  0.32180  0.29830  0.1563   \n",
       "1152807  0.21990  0.25540  0.25480  0.25390  0.27920  0.1567   \n",
       "1152808  0.27290  0.30900  0.30940  0.29640  0.30290  0.1617   \n",
       "1152809  0.33570  0.38040  0.39880  0.40650  0.28320  0.1495   \n",
       "\n",
       "         sample_location_id  classes      NDVI       EVI      NDWI      SAVI  \\\n",
       "0                    201701        2  0.568914  0.246737 -0.052386  0.271183   \n",
       "1                    201701        2  0.581259  0.287926 -0.015721  0.308070   \n",
       "2                    201701        2  0.834221  0.731688  0.337125  0.652436   \n",
       "3                    201701        2  0.776236  0.581962  0.247564  0.544852   \n",
       "4                    201701        2  0.830017  0.746039  0.372872  0.664865   \n",
       "...                     ...      ...       ...       ...       ...       ...   \n",
       "1152805              202312        2  0.516930  0.319632 -0.031002  0.321629   \n",
       "1152806              202312        2  0.625819  0.443384  0.076328  0.432730   \n",
       "1152807              202312        2  0.484021  0.294097 -0.044519  0.296020   \n",
       "1152808              202312        2  0.602697  0.397850  0.009969  0.393631   \n",
       "1152809              202312        2  0.674664  0.506344  0.146474  0.481767   \n",
       "\n",
       "            GNDVI  \n",
       "0        0.670018  \n",
       "1        0.688819  \n",
       "2        0.765506  \n",
       "3        0.721622  \n",
       "4        0.767403  \n",
       "...           ...  \n",
       "1152805  0.602862  \n",
       "1152806  0.637691  \n",
       "1152807  0.571209  \n",
       "1152808  0.653291  \n",
       "1152809  0.652476  \n",
       "\n",
       "[1152810 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.read_csv('merged_df.csv')\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>sample_location_id</th>\n",
       "      <th>classes</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05020</td>\n",
       "      <td>0.09065</td>\n",
       "      <td>0.13930</td>\n",
       "      <td>0.15855</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.19205</td>\n",
       "      <td>0.19175</td>\n",
       "      <td>0.20290</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.568914</td>\n",
       "      <td>0.246737</td>\n",
       "      <td>-0.052386</td>\n",
       "      <td>0.271183</td>\n",
       "      <td>0.670018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.03980</td>\n",
       "      <td>0.05720</td>\n",
       "      <td>0.10385</td>\n",
       "      <td>0.16755</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.21600</td>\n",
       "      <td>0.23420</td>\n",
       "      <td>0.24700</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.581259</td>\n",
       "      <td>0.287926</td>\n",
       "      <td>-0.015721</td>\n",
       "      <td>0.308070</td>\n",
       "      <td>0.688819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01510</td>\n",
       "      <td>0.02905</td>\n",
       "      <td>0.06635</td>\n",
       "      <td>0.04515</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.38280</td>\n",
       "      <td>0.47410</td>\n",
       "      <td>0.49955</td>\n",
       "      <td>0.50775</td>\n",
       "      <td>0.50890</td>\n",
       "      <td>0.24765</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.834221</td>\n",
       "      <td>0.731688</td>\n",
       "      <td>0.337125</td>\n",
       "      <td>0.652436</td>\n",
       "      <td>0.765506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.06315</td>\n",
       "      <td>0.04920</td>\n",
       "      <td>0.12335</td>\n",
       "      <td>0.28515</td>\n",
       "      <td>0.33710</td>\n",
       "      <td>0.39055</td>\n",
       "      <td>0.37655</td>\n",
       "      <td>0.42050</td>\n",
       "      <td>0.23555</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.776236</td>\n",
       "      <td>0.581962</td>\n",
       "      <td>0.247564</td>\n",
       "      <td>0.544852</td>\n",
       "      <td>0.721622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.02970</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.13360</td>\n",
       "      <td>0.39940</td>\n",
       "      <td>0.49485</td>\n",
       "      <td>0.52430</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.48935</td>\n",
       "      <td>0.23950</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>201701</td>\n",
       "      <td>2</td>\n",
       "      <td>0.830017</td>\n",
       "      <td>0.746039</td>\n",
       "      <td>0.372872</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.767403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152805</th>\n",
       "      <td>1152806</td>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.04660</td>\n",
       "      <td>0.06660</td>\n",
       "      <td>0.08560</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.26880</td>\n",
       "      <td>0.28690</td>\n",
       "      <td>0.27960</td>\n",
       "      <td>0.28600</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.516930</td>\n",
       "      <td>0.319632</td>\n",
       "      <td>-0.031002</td>\n",
       "      <td>0.321629</td>\n",
       "      <td>0.602862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152806</th>\n",
       "      <td>1152807</td>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.07690</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.14000</td>\n",
       "      <td>0.25190</td>\n",
       "      <td>0.28530</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>0.33920</td>\n",
       "      <td>0.32180</td>\n",
       "      <td>0.29830</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625819</td>\n",
       "      <td>0.443384</td>\n",
       "      <td>0.076328</td>\n",
       "      <td>0.432730</td>\n",
       "      <td>0.637691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152807</th>\n",
       "      <td>1152808</td>\n",
       "      <td>0.04280</td>\n",
       "      <td>0.04960</td>\n",
       "      <td>0.06970</td>\n",
       "      <td>0.08880</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.18960</td>\n",
       "      <td>0.21990</td>\n",
       "      <td>0.25540</td>\n",
       "      <td>0.25480</td>\n",
       "      <td>0.25390</td>\n",
       "      <td>0.27920</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484021</td>\n",
       "      <td>0.294097</td>\n",
       "      <td>-0.044519</td>\n",
       "      <td>0.296020</td>\n",
       "      <td>0.571209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152808</th>\n",
       "      <td>1152809</td>\n",
       "      <td>0.03170</td>\n",
       "      <td>0.04110</td>\n",
       "      <td>0.06480</td>\n",
       "      <td>0.07660</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>0.23340</td>\n",
       "      <td>0.27290</td>\n",
       "      <td>0.30900</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.29640</td>\n",
       "      <td>0.30290</td>\n",
       "      <td>0.1617</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.602697</td>\n",
       "      <td>0.397850</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.393631</td>\n",
       "      <td>0.653291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152809</th>\n",
       "      <td>1152810</td>\n",
       "      <td>0.03540</td>\n",
       "      <td>0.04140</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.28990</td>\n",
       "      <td>0.33570</td>\n",
       "      <td>0.38040</td>\n",
       "      <td>0.39880</td>\n",
       "      <td>0.40650</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>202312</td>\n",
       "      <td>2</td>\n",
       "      <td>0.674664</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.146474</td>\n",
       "      <td>0.481767</td>\n",
       "      <td>0.652476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152810 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0       B1       B2       B3       B4       B5       B6  \\\n",
       "0                 0  0.00495  0.01885  0.03610  0.05020  0.09065  0.13930   \n",
       "1                 1  0.01040  0.02405  0.03980  0.05720  0.10385  0.16755   \n",
       "2                 2  0.01510  0.02905  0.06635  0.04515  0.12920  0.38280   \n",
       "3                 3  0.01345  0.02925  0.06315  0.04920  0.12335  0.28515   \n",
       "4                 4  0.01575  0.02970  0.06900  0.04870  0.13360  0.39940   \n",
       "...             ...      ...      ...      ...      ...      ...      ...   \n",
       "1152805     1152806  0.03720  0.04660  0.06660  0.08560  0.13300  0.21200   \n",
       "1152806     1152807  0.03840  0.04250  0.07690  0.08000  0.14000  0.25190   \n",
       "1152807     1152808  0.04280  0.04960  0.06970  0.08880  0.12820  0.18960   \n",
       "1152808     1152809  0.03170  0.04110  0.06480  0.07660  0.13350  0.23340   \n",
       "1152809     1152810  0.03540  0.04140  0.08000  0.07390  0.15220  0.28990   \n",
       "\n",
       "              B7       B8      B8A       B9      B11     B12  \\\n",
       "0        0.15855  0.18270  0.19205  0.19175  0.20290  0.1097   \n",
       "1        0.19370  0.21600  0.23420  0.24700  0.22290  0.1204   \n",
       "2        0.47410  0.49955  0.50775  0.50890  0.24765  0.1219   \n",
       "3        0.33710  0.39055  0.37655  0.42050  0.23555  0.1175   \n",
       "4        0.49485  0.52430  0.53900  0.48935  0.23950  0.1177   \n",
       "...          ...      ...      ...      ...      ...     ...   \n",
       "1152805  0.24360  0.26880  0.28690  0.27960  0.28600  0.1640   \n",
       "1152806  0.28530  0.34760  0.33920  0.32180  0.29830  0.1563   \n",
       "1152807  0.21990  0.25540  0.25480  0.25390  0.27920  0.1567   \n",
       "1152808  0.27290  0.30900  0.30940  0.29640  0.30290  0.1617   \n",
       "1152809  0.33570  0.38040  0.39880  0.40650  0.28320  0.1495   \n",
       "\n",
       "         sample_location_id  classes      NDVI       EVI      NDWI      SAVI  \\\n",
       "0                    201701        2  0.568914  0.246737 -0.052386  0.271183   \n",
       "1                    201701        2  0.581259  0.287926 -0.015721  0.308070   \n",
       "2                    201701        2  0.834221  0.731688  0.337125  0.652436   \n",
       "3                    201701        2  0.776236  0.581962  0.247564  0.544852   \n",
       "4                    201701        2  0.830017  0.746039  0.372872  0.664865   \n",
       "...                     ...      ...       ...       ...       ...       ...   \n",
       "1152805              202312        2  0.516930  0.319632 -0.031002  0.321629   \n",
       "1152806              202312        2  0.625819  0.443384  0.076328  0.432730   \n",
       "1152807              202312        2  0.484021  0.294097 -0.044519  0.296020   \n",
       "1152808              202312        2  0.602697  0.397850  0.009969  0.393631   \n",
       "1152809              202312        2  0.674664  0.506344  0.146474  0.481767   \n",
       "\n",
       "            GNDVI  \n",
       "0        0.670018  \n",
       "1        0.688819  \n",
       "2        0.765506  \n",
       "3        0.721622  \n",
       "4        0.767403  \n",
       "...           ...  \n",
       "1152805  0.602862  \n",
       "1152806  0.637691  \n",
       "1152807  0.571209  \n",
       "1152808  0.653291  \n",
       "1152809  0.652476  \n",
       "\n",
       "[1152810 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['NDVI'] = (merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + merged_df['B4'])\n",
    "\n",
    "# EVI is an optimized vegetation index designed to enhance the vegetation signal with improved sensitivity in high biomass regions\n",
    "# It's calculated using the Red (B4), Near-Infrared (B8 or B5), and Blue (B2) bands.\n",
    "merged_df['EVI'] = 2.5 * (merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + 6 * merged_df['B4'] - 7.5 * merged_df['B2'] + 1)\n",
    "\n",
    "# NDWI is used to monitor changes in water content of leaves\n",
    "# It is typically calculated using the Near-Infrared (B8 or B5) and Short-Wave Infrared (B11 or B6) bands.\n",
    "merged_df['NDWI'] = (merged_df['B8'] - merged_df['B11']) / (merged_df['B8'] + merged_df['B11'])\n",
    "\n",
    "# SAVI is a modification of NDVI to correct for the influence of soil brightness\n",
    "# The standard value of L in the SAVI formula is 0.5.\n",
    "L = 0.5  # soil brightness correction factor\n",
    "merged_df['SAVI'] = ((merged_df['B8'] - merged_df['B4']) / (merged_df['B8'] + merged_df['B4'] + L)) * (1 + L)\n",
    "\n",
    "# GNDVI is used to estimate vegetation health\n",
    "# It's calculated using the Near-Infrared (B8 or B5) and Green (B3) bands.\n",
    "merged_df['GNDVI'] = (merged_df['B8'] - merged_df['B3']) / (merged_df['B8'] + merged_df['B3'])\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming 'class' column to 'classes' to fix python error\n",
    "merged_df = merged_df.rename(columns={'class': 'classes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, I will make some new dataframes for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a DataFrame with classes 1 and 4\n",
    "class_1_4_df = merged_df[merged_df['classes'].isin([1, 4])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>classes</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9842</th>\n",
       "      <td>0.01865</td>\n",
       "      <td>0.05105</td>\n",
       "      <td>0.07580</td>\n",
       "      <td>0.06180</td>\n",
       "      <td>0.12655</td>\n",
       "      <td>0.27940</td>\n",
       "      <td>0.32480</td>\n",
       "      <td>0.35875</td>\n",
       "      <td>0.37050</td>\n",
       "      <td>0.35395</td>\n",
       "      <td>0.20805</td>\n",
       "      <td>0.10675</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706099</td>\n",
       "      <td>0.551265</td>\n",
       "      <td>0.265879</td>\n",
       "      <td>0.483868</td>\n",
       "      <td>0.651133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9843</th>\n",
       "      <td>0.02580</td>\n",
       "      <td>0.05455</td>\n",
       "      <td>0.07065</td>\n",
       "      <td>0.06045</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.26640</td>\n",
       "      <td>0.30535</td>\n",
       "      <td>0.33880</td>\n",
       "      <td>0.34985</td>\n",
       "      <td>0.33795</td>\n",
       "      <td>0.22375</td>\n",
       "      <td>0.11440</td>\n",
       "      <td>1</td>\n",
       "      <td>0.697182</td>\n",
       "      <td>0.538447</td>\n",
       "      <td>0.204515</td>\n",
       "      <td>0.464304</td>\n",
       "      <td>0.654903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9844</th>\n",
       "      <td>0.01355</td>\n",
       "      <td>0.03450</td>\n",
       "      <td>0.06355</td>\n",
       "      <td>0.05075</td>\n",
       "      <td>0.11345</td>\n",
       "      <td>0.24090</td>\n",
       "      <td>0.27965</td>\n",
       "      <td>0.30590</td>\n",
       "      <td>0.32365</td>\n",
       "      <td>0.31530</td>\n",
       "      <td>0.18985</td>\n",
       "      <td>0.10140</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715407</td>\n",
       "      <td>0.471923</td>\n",
       "      <td>0.234090</td>\n",
       "      <td>0.446769</td>\n",
       "      <td>0.655975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9845</th>\n",
       "      <td>0.02700</td>\n",
       "      <td>0.04475</td>\n",
       "      <td>0.07310</td>\n",
       "      <td>0.07660</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.27565</td>\n",
       "      <td>0.32250</td>\n",
       "      <td>0.34800</td>\n",
       "      <td>0.36675</td>\n",
       "      <td>0.36475</td>\n",
       "      <td>0.26760</td>\n",
       "      <td>0.15715</td>\n",
       "      <td>1</td>\n",
       "      <td>0.639190</td>\n",
       "      <td>0.460945</td>\n",
       "      <td>0.130604</td>\n",
       "      <td>0.440299</td>\n",
       "      <td>0.652814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9846</th>\n",
       "      <td>0.02190</td>\n",
       "      <td>0.04735</td>\n",
       "      <td>0.06520</td>\n",
       "      <td>0.05325</td>\n",
       "      <td>0.11545</td>\n",
       "      <td>0.26510</td>\n",
       "      <td>0.30440</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.34915</td>\n",
       "      <td>0.35330</td>\n",
       "      <td>0.21820</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.719552</td>\n",
       "      <td>0.529195</td>\n",
       "      <td>0.198825</td>\n",
       "      <td>0.465899</td>\n",
       "      <td>0.667092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107630</th>\n",
       "      <td>0.03940</td>\n",
       "      <td>0.04150</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>0.07980</td>\n",
       "      <td>0.12370</td>\n",
       "      <td>0.19380</td>\n",
       "      <td>0.22340</td>\n",
       "      <td>0.24700</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>0.27200</td>\n",
       "      <td>0.27330</td>\n",
       "      <td>0.15590</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>-0.050548</td>\n",
       "      <td>0.303338</td>\n",
       "      <td>0.581306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107631</th>\n",
       "      <td>0.03630</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>0.05900</td>\n",
       "      <td>0.07600</td>\n",
       "      <td>0.12830</td>\n",
       "      <td>0.20520</td>\n",
       "      <td>0.23680</td>\n",
       "      <td>0.25780</td>\n",
       "      <td>0.28190</td>\n",
       "      <td>0.29500</td>\n",
       "      <td>0.27540</td>\n",
       "      <td>0.15850</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544638</td>\n",
       "      <td>0.317433</td>\n",
       "      <td>-0.033008</td>\n",
       "      <td>0.327057</td>\n",
       "      <td>0.627525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107632</th>\n",
       "      <td>0.02560</td>\n",
       "      <td>0.03560</td>\n",
       "      <td>0.05040</td>\n",
       "      <td>0.06380</td>\n",
       "      <td>0.10790</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.20570</td>\n",
       "      <td>0.22860</td>\n",
       "      <td>0.24280</td>\n",
       "      <td>0.26940</td>\n",
       "      <td>0.27770</td>\n",
       "      <td>0.15700</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563611</td>\n",
       "      <td>0.306456</td>\n",
       "      <td>-0.096978</td>\n",
       "      <td>0.311964</td>\n",
       "      <td>0.638710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107633</th>\n",
       "      <td>0.03450</td>\n",
       "      <td>0.04550</td>\n",
       "      <td>0.07100</td>\n",
       "      <td>0.08820</td>\n",
       "      <td>0.13700</td>\n",
       "      <td>0.23200</td>\n",
       "      <td>0.26630</td>\n",
       "      <td>0.28900</td>\n",
       "      <td>0.30960</td>\n",
       "      <td>0.31320</td>\n",
       "      <td>0.25640</td>\n",
       "      <td>0.15010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532344</td>\n",
       "      <td>0.339890</td>\n",
       "      <td>0.059773</td>\n",
       "      <td>0.343365</td>\n",
       "      <td>0.605556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107634</th>\n",
       "      <td>0.02840</td>\n",
       "      <td>0.04320</td>\n",
       "      <td>0.06640</td>\n",
       "      <td>0.08240</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.23620</td>\n",
       "      <td>0.27420</td>\n",
       "      <td>0.28860</td>\n",
       "      <td>0.34640</td>\n",
       "      <td>0.34400</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.16360</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555795</td>\n",
       "      <td>0.353324</td>\n",
       "      <td>-0.023681</td>\n",
       "      <td>0.355109</td>\n",
       "      <td>0.625915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744364 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              B1       B2       B3       B4       B5       B6       B7  \\\n",
       "9842     0.01865  0.05105  0.07580  0.06180  0.12655  0.27940  0.32480   \n",
       "9843     0.02580  0.05455  0.07065  0.06045  0.12780  0.26640  0.30535   \n",
       "9844     0.01355  0.03450  0.06355  0.05075  0.11345  0.24090  0.27965   \n",
       "9845     0.02700  0.04475  0.07310  0.07660  0.14820  0.27565  0.32250   \n",
       "9846     0.02190  0.04735  0.06520  0.05325  0.11545  0.26510  0.30440   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "1107630  0.03940  0.04150  0.06540  0.07980  0.12370  0.19380  0.22340   \n",
       "1107631  0.03630  0.03760  0.05900  0.07600  0.12830  0.20520  0.23680   \n",
       "1107632  0.02560  0.03560  0.05040  0.06380  0.10790  0.18100  0.20570   \n",
       "1107633  0.03450  0.04550  0.07100  0.08820  0.13700  0.23200  0.26630   \n",
       "1107634  0.02840  0.04320  0.06640  0.08240  0.14130  0.23620  0.27420   \n",
       "\n",
       "              B8      B8A       B9      B11      B12  classes      NDVI  \\\n",
       "9842     0.35875  0.37050  0.35395  0.20805  0.10675        1  0.706099   \n",
       "9843     0.33880  0.34985  0.33795  0.22375  0.11440        1  0.697182   \n",
       "9844     0.30590  0.32365  0.31530  0.18985  0.10140        1  0.715407   \n",
       "9845     0.34800  0.36675  0.36475  0.26760  0.15715        1  0.639190   \n",
       "9846     0.32650  0.34915  0.35330  0.21820  0.11260        1  0.719552   \n",
       "...          ...      ...      ...      ...      ...      ...       ...   \n",
       "1107630  0.24700  0.26710  0.27200  0.27330  0.15590        1  0.511628   \n",
       "1107631  0.25780  0.28190  0.29500  0.27540  0.15850        1  0.544638   \n",
       "1107632  0.22860  0.24280  0.26940  0.27770  0.15700        1  0.563611   \n",
       "1107633  0.28900  0.30960  0.31320  0.25640  0.15010        1  0.532344   \n",
       "1107634  0.28860  0.34640  0.34400  0.30260  0.16360        1  0.555795   \n",
       "\n",
       "              EVI      NDWI      SAVI     GNDVI  \n",
       "9842     0.551265  0.265879  0.483868  0.651133  \n",
       "9843     0.538447  0.204515  0.464304  0.654903  \n",
       "9844     0.471923  0.234090  0.446769  0.655975  \n",
       "9845     0.460945  0.130604  0.440299  0.652814  \n",
       "9846     0.529195  0.198825  0.465899  0.667092  \n",
       "...           ...       ...       ...       ...  \n",
       "1107630  0.295500 -0.050548  0.303338  0.581306  \n",
       "1107631  0.317433 -0.033008  0.327057  0.627525  \n",
       "1107632  0.306456 -0.096978  0.311964  0.638710  \n",
       "1107633  0.339890  0.059773  0.343365  0.605556  \n",
       "1107634  0.353324 -0.023681  0.355109  0.625915  \n",
       "\n",
       "[744364 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the columns you want to keep\n",
    "class_1_4_df = class_1_4_df.loc[:, ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A',\n",
    "                              'B9', 'B11', 'B12', 'classes', 'NDVI', 'EVI', 'NDWI', 'SAVI', 'GNDVI']]\n",
    "class_1_4_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing train and test sets\n",
    "\n",
    "First shuffling dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B9</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>classes</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>GNDVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03425</td>\n",
       "      <td>0.03845</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.06870</td>\n",
       "      <td>0.13100</td>\n",
       "      <td>0.20070</td>\n",
       "      <td>0.23280</td>\n",
       "      <td>0.25870</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.26380</td>\n",
       "      <td>0.24960</td>\n",
       "      <td>0.13535</td>\n",
       "      <td>4</td>\n",
       "      <td>0.580330</td>\n",
       "      <td>0.343574</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.344453</td>\n",
       "      <td>0.649872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03250</td>\n",
       "      <td>0.04340</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0.06380</td>\n",
       "      <td>0.14050</td>\n",
       "      <td>0.26550</td>\n",
       "      <td>0.30340</td>\n",
       "      <td>0.31760</td>\n",
       "      <td>0.35510</td>\n",
       "      <td>0.31620</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.12960</td>\n",
       "      <td>1</td>\n",
       "      <td>0.665443</td>\n",
       "      <td>0.461488</td>\n",
       "      <td>0.131256</td>\n",
       "      <td>0.431926</td>\n",
       "      <td>0.663263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03775</td>\n",
       "      <td>0.05730</td>\n",
       "      <td>0.0808</td>\n",
       "      <td>0.09515</td>\n",
       "      <td>0.14805</td>\n",
       "      <td>0.22945</td>\n",
       "      <td>0.25975</td>\n",
       "      <td>0.30075</td>\n",
       "      <td>0.29675</td>\n",
       "      <td>0.28945</td>\n",
       "      <td>0.25910</td>\n",
       "      <td>0.14280</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519323</td>\n",
       "      <td>0.356474</td>\n",
       "      <td>0.074395</td>\n",
       "      <td>0.344235</td>\n",
       "      <td>0.576464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03440</td>\n",
       "      <td>0.04540</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.06720</td>\n",
       "      <td>0.15140</td>\n",
       "      <td>0.30720</td>\n",
       "      <td>0.36230</td>\n",
       "      <td>0.39480</td>\n",
       "      <td>0.41350</td>\n",
       "      <td>0.42180</td>\n",
       "      <td>0.23620</td>\n",
       "      <td>0.12740</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.561921</td>\n",
       "      <td>0.251347</td>\n",
       "      <td>0.510811</td>\n",
       "      <td>0.682864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03710</td>\n",
       "      <td>0.04320</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>0.07140</td>\n",
       "      <td>0.13600</td>\n",
       "      <td>0.27470</td>\n",
       "      <td>0.32090</td>\n",
       "      <td>0.35600</td>\n",
       "      <td>0.36920</td>\n",
       "      <td>0.42830</td>\n",
       "      <td>0.23130</td>\n",
       "      <td>0.12350</td>\n",
       "      <td>1</td>\n",
       "      <td>0.665887</td>\n",
       "      <td>0.487195</td>\n",
       "      <td>0.212328</td>\n",
       "      <td>0.460319</td>\n",
       "      <td>0.670185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744359</th>\n",
       "      <td>0.03420</td>\n",
       "      <td>0.04950</td>\n",
       "      <td>0.0728</td>\n",
       "      <td>0.06050</td>\n",
       "      <td>0.14080</td>\n",
       "      <td>0.29350</td>\n",
       "      <td>0.34160</td>\n",
       "      <td>0.37720</td>\n",
       "      <td>0.39280</td>\n",
       "      <td>0.36720</td>\n",
       "      <td>0.25400</td>\n",
       "      <td>0.12470</td>\n",
       "      <td>1</td>\n",
       "      <td>0.723555</td>\n",
       "      <td>0.578363</td>\n",
       "      <td>0.195184</td>\n",
       "      <td>0.506612</td>\n",
       "      <td>0.676444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744360</th>\n",
       "      <td>0.03510</td>\n",
       "      <td>0.04450</td>\n",
       "      <td>0.0628</td>\n",
       "      <td>0.06800</td>\n",
       "      <td>0.11520</td>\n",
       "      <td>0.18780</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.24460</td>\n",
       "      <td>0.25600</td>\n",
       "      <td>0.27930</td>\n",
       "      <td>0.21960</td>\n",
       "      <td>0.11240</td>\n",
       "      <td>1</td>\n",
       "      <td>0.564939</td>\n",
       "      <td>0.334761</td>\n",
       "      <td>0.053856</td>\n",
       "      <td>0.325991</td>\n",
       "      <td>0.591412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744361</th>\n",
       "      <td>0.02590</td>\n",
       "      <td>0.03070</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.04920</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.20140</td>\n",
       "      <td>0.23210</td>\n",
       "      <td>0.25840</td>\n",
       "      <td>0.27360</td>\n",
       "      <td>0.30400</td>\n",
       "      <td>0.21370</td>\n",
       "      <td>0.10680</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680104</td>\n",
       "      <td>0.395209</td>\n",
       "      <td>0.094683</td>\n",
       "      <td>0.388559</td>\n",
       "      <td>0.682839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744362</th>\n",
       "      <td>0.03420</td>\n",
       "      <td>0.04840</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.08080</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.19350</td>\n",
       "      <td>0.22380</td>\n",
       "      <td>0.24400</td>\n",
       "      <td>0.26620</td>\n",
       "      <td>0.26820</td>\n",
       "      <td>0.27060</td>\n",
       "      <td>0.15340</td>\n",
       "      <td>1</td>\n",
       "      <td>0.502463</td>\n",
       "      <td>0.298726</td>\n",
       "      <td>-0.051691</td>\n",
       "      <td>0.296799</td>\n",
       "      <td>0.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744363</th>\n",
       "      <td>0.03520</td>\n",
       "      <td>0.05185</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.07425</td>\n",
       "      <td>0.12295</td>\n",
       "      <td>0.19440</td>\n",
       "      <td>0.21780</td>\n",
       "      <td>0.23920</td>\n",
       "      <td>0.25565</td>\n",
       "      <td>0.25655</td>\n",
       "      <td>0.25175</td>\n",
       "      <td>0.14095</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526240</td>\n",
       "      <td>0.318234</td>\n",
       "      <td>-0.025563</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.559830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744364 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             B1       B2      B3       B4       B5       B6       B7       B8  \\\n",
       "0       0.03425  0.03845  0.0549  0.06870  0.13100  0.20070  0.23280  0.25870   \n",
       "1       0.03250  0.04340  0.0643  0.06380  0.14050  0.26550  0.30340  0.31760   \n",
       "2       0.03775  0.05730  0.0808  0.09515  0.14805  0.22945  0.25975  0.30075   \n",
       "3       0.03440  0.04540  0.0744  0.06720  0.15140  0.30720  0.36230  0.39480   \n",
       "4       0.03710  0.04320  0.0703  0.07140  0.13600  0.27470  0.32090  0.35600   \n",
       "...         ...      ...     ...      ...      ...      ...      ...      ...   \n",
       "744359  0.03420  0.04950  0.0728  0.06050  0.14080  0.29350  0.34160  0.37720   \n",
       "744360  0.03510  0.04450  0.0628  0.06800  0.11520  0.18780  0.21130  0.24460   \n",
       "744361  0.02590  0.03070  0.0487  0.04920  0.10770  0.20140  0.23210  0.25840   \n",
       "744362  0.03420  0.04840  0.0686  0.08080  0.13400  0.19350  0.22380  0.24400   \n",
       "744363  0.03520  0.05185  0.0675  0.07425  0.12295  0.19440  0.21780  0.23920   \n",
       "\n",
       "            B8A       B9      B11      B12  classes      NDVI       EVI  \\\n",
       "0       0.27735  0.26380  0.24960  0.13535        4  0.580330  0.343574   \n",
       "1       0.35510  0.31620  0.24390  0.12960        1  0.665443  0.461488   \n",
       "2       0.29675  0.28945  0.25910  0.14280        1  0.519323  0.356474   \n",
       "3       0.41350  0.42180  0.23620  0.12740        1  0.709091  0.561921   \n",
       "4       0.36920  0.42830  0.23130  0.12350        1  0.665887  0.487195   \n",
       "...         ...      ...      ...      ...      ...       ...       ...   \n",
       "744359  0.39280  0.36720  0.25400  0.12470        1  0.723555  0.578363   \n",
       "744360  0.25600  0.27930  0.21960  0.11240        1  0.564939  0.334761   \n",
       "744361  0.27360  0.30400  0.21370  0.10680        1  0.680104  0.395209   \n",
       "744362  0.26620  0.26820  0.27060  0.15340        1  0.502463  0.298726   \n",
       "744363  0.25565  0.25655  0.25175  0.14095        1  0.526240  0.318234   \n",
       "\n",
       "            NDWI      SAVI     GNDVI  \n",
       "0       0.017903  0.344453  0.649872  \n",
       "1       0.131256  0.431926  0.663263  \n",
       "2       0.074395  0.344235  0.576464  \n",
       "3       0.251347  0.510811  0.682864  \n",
       "4       0.212328  0.460319  0.670185  \n",
       "...          ...       ...       ...  \n",
       "744359  0.195184  0.506612  0.676444  \n",
       "744360  0.053856  0.325991  0.591412  \n",
       "744361  0.094683  0.388559  0.682839  \n",
       "744362 -0.051691  0.296799  0.561100  \n",
       "744363 -0.025563  0.304167  0.559830  \n",
       "\n",
       "[744364 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the DataFrame using a random seed, for example, seed=42\n",
    "class_1_4_df = class_1_4_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "class_1_4_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then making splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your features are all columns except 'classes', and 'classes' is the target variable\n",
    "X = class_1_4_df.drop('classes', axis=1)  # Features\n",
    "y = class_1_4_df['classes']  # Target variable\n",
    "\n",
    "# Perform the split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# X_train and y_train will now contain 70% of the data, X_test and y_test will contain 30%\n",
    "# Both splits will have the same proportion of class 0 and 4 as the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 400)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    # Ensuring min_samples_split is an int >= 2\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    # Ensuring min_samples_leaf is a float within (0.0, 0.5], you could also use suggest_int if you want specific integer values\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
    "\n",
    "    # Initialize the classifier with the current hyperparameters\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    # Compute and return the accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-02-12 17:39:46,546]\u001b[0m A new study created in memory with name: no-name-55e44e6e-0f70-434c-a6db-5942327c8598\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 17:42:59,345]\u001b[0m Trial 0 finished with value: 0.7529174689892973 and parameters: {'n_estimators': 150, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.7529174689892973.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 17:46:15,663]\u001b[0m Trial 1 finished with value: 0.7161569119161704 and parameters: {'n_estimators': 259, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.7529174689892973.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 17:55:48,936]\u001b[0m Trial 2 finished with value: 0.829738927947696 and parameters: {'n_estimators': 306, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.829738927947696.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:04:07,574]\u001b[0m Trial 3 finished with value: 0.8692266356186468 and parameters: {'n_estimators': 210, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.8692266356186468.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:12:54,851]\u001b[0m Trial 4 finished with value: 0.9128431328646276 and parameters: {'n_estimators': 205, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 4 with value: 0.9128431328646276.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:15:43,351]\u001b[0m Trial 5 finished with value: 0.8558998701356858 and parameters: {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 4 with value: 0.9128431328646276.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:26:35,203]\u001b[0m Trial 6 finished with value: 0.9098249070798442 and parameters: {'n_estimators': 326, 'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 4 with value: 0.9128431328646276.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:29:02,708]\u001b[0m Trial 7 finished with value: 0.7100935918678071 and parameters: {'n_estimators': 347, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.9128431328646276.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:32:19,035]\u001b[0m Trial 8 finished with value: 0.7983923693520218 and parameters: {'n_estimators': 159, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 4 with value: 0.9128431328646276.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:32:43,804]\u001b[0m Trial 9 finished with value: 0.6960189870583494 and parameters: {'n_estimators': 89, 'max_depth': 2, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.9128431328646276.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:49:16,306]\u001b[0m Trial 10 finished with value: 0.9355201289686982 and parameters: {'n_estimators': 396, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9355201289686982.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 18:58:53,602]\u001b[0m Trial 11 finished with value: 0.9370919349782814 and parameters: {'n_estimators': 245, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.9370919349782814.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 19:13:21,921]\u001b[0m Trial 12 finished with value: 0.9383144507635126 and parameters: {'n_estimators': 379, 'max_depth': 32, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.9383144507635126.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 19:28:12,479]\u001b[0m Trial 13 finished with value: 0.9421655993909811 and parameters: {'n_estimators': 398, 'max_depth': 31, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.9421655993909811.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 19:33:19,309]\u001b[0m Trial 14 finished with value: 0.7247145224127894 and parameters: {'n_estimators': 389, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.9421655993909811.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 19:46:22,288]\u001b[0m Trial 15 finished with value: 0.9421252966727867 and parameters: {'n_estimators': 358, 'max_depth': 31, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.9421655993909811.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 19:55:49,422]\u001b[0m Trial 16 finished with value: 0.9135058886749362 and parameters: {'n_estimators': 290, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.9421655993909811.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 20:00:25,398]\u001b[0m Trial 17 finished with value: 0.7246070484976043 and parameters: {'n_estimators': 349, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.9421655993909811.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 20:12:49,659]\u001b[0m Trial 18 finished with value: 0.9255071425372801 and parameters: {'n_estimators': 360, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.9421655993909811.\u001b[0m\n",
      "\u001b[32m[I 2024-02-12 20:22:24,522]\u001b[0m Trial 19 finished with value: 0.8815906139447405 and parameters: {'n_estimators': 291, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.9421655993909811.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'n_estimators': 398, 'max_depth': 31, 'min_samples_split': 2, 'min_samples_leaf': 2}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First using a logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "Precision: 0.6892590098857939\n",
      "Logistic Regression Accuracy: 0.7051856164076844\n",
      "Recall: 0.7051856164076844\n",
      "F1 Score: 0.668430491744793\n",
      "Cohen's Kappa: 0.23646225680367183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "log_reg = LogisticRegression(max_iter=1000) # Increase max_iter if convergence warnings occur\n",
    "\n",
    "# Train the classifier on the training set\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "log_reg_predictions = log_reg.predict(X_test)\n",
    "\n",
    "# Predict on the test set\n",
    "lr_predictions = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics, specifying pos_label for binary classification\n",
    "lr_precision = precision_score(y_test, lr_predictions, average='weighted')\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions)\n",
    "lr_recall = recall_score(y_test, lr_predictions, average='weighted')\n",
    "lr_f1 = f1_score(y_test, lr_predictions, average='weighted')\n",
    "lr_kappa = cohen_kappa_score(y_test, lr_predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(f\"Precision: {lr_precision}\")\n",
    "print(f'Logistic Regression Accuracy: {log_reg_accuracy}')\n",
    "print(f\"Recall: {lr_recall}\")\n",
    "print(f\"F1 Score: {lr_f1}\")\n",
    "print(f\"Cohen's Kappa: {lr_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then an RF model based on optimised hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Performance:\n",
      "Precision: 0.9427567110253074\n",
      "Recall: 0.9421655993909811\n",
      "F1 Score: 0.9414287923103802\n",
      "Cohen's Kappa: 0.8672107740250168\n",
      "Optimized RandomForest Accuracy: 0.9421655993909811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Best trial: {'n_estimators': 398, 'max_depth': 31, 'min_samples_split': 2, 'min_samples_leaf': 2}\n",
    "\n",
    "# Assuming you have your optimized hyperparameters, for example:\n",
    "optimized_hyperparameters = {\n",
    "    'n_estimators': 398,\n",
    "    'max_depth': 31,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 2, \n",
    "    # Include other hyperparameters as necessary\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with optimized hyperparameters\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=optimized_hyperparameters['n_estimators'],\n",
    "    max_depth=optimized_hyperparameters['max_depth'],\n",
    "    min_samples_split=optimized_hyperparameters['min_samples_split'],\n",
    "    min_samples_leaf=optimized_hyperparameters['min_samples_leaf'],\n",
    "    random_state=42  # Ensuring reproducibility\n",
    ")\n",
    "\n",
    "# Train the classifier on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Compute and display the accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "rf_precision = precision_score(y_test, predictions, average='weighted')\n",
    "rf_recall = recall_score(y_test, predictions, average='weighted')\n",
    "rf_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "rf_kappa = cohen_kappa_score(y_test, predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(\"RandomForest Performance:\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1 Score: {rf_f1}\")\n",
    "print(f\"Cohen's Kappa: {rf_kappa}\")\n",
    "print(f'Optimized RandomForest Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You'll also want to output an aggregated prediction confusion matrix (from the cross-validation), preferably as a seaborn (sns) figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming predictions, y_test are already defined\n",
    "\n",
    "# Compute the normalized confusion matrix\n",
    "cm_normalized = confusion_matrix(y_test, predictions, labels=[1, 4], normalize='true')\n",
    "\n",
    "# Visualize the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\", xticklabels=[1, 4], yticklabels=[1, 4])\n",
    "plt.title('Normalised Confusion Matrix Round 4 No. 8')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Save the figure with a white background\n",
    "plt.savefig('round4_no8_matrix.png', bbox_inches='tight', pad_inches=0, facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming clf is your trained RandomForestClassifier model\n",
    "# and X_test, y_test are your test datasets\n",
    "\n",
    "result = permutation_importance(clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Sorting features by importance\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "bp = ax.boxplot(result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx],\n",
    "                patch_artist=True,  # To fill with color\n",
    "                )\n",
    "\n",
    "# Customizing the boxplot color to dark blue\n",
    "for box in bp['boxes']:\n",
    "    # Change box color\n",
    "    box.set(color='darkblue', linewidth=2)  # Box edge color\n",
    "    box.set(facecolor='darkblue')  # Box fill color\n",
    "\n",
    "# Optionally, customize whiskers, fliers, caps, and medians if needed\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='darkblue', linewidth=2)\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='darkblue', linewidth=2)\n",
    "for median in bp['medians']:\n",
    "    median.set(color='gold', linewidth=2)  # Making the median stand out\n",
    "\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_title(\"Permutation Importances Round 4 No. 8 (test set)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('r4_n8_features.png', bbox_inches='tight', pad_inches=0, facecolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
